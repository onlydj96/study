{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>전일비</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>등락률</th>\n",
       "      <th>거래량</th>\n",
       "      <th>금액(백만)</th>\n",
       "      <th>신용비</th>\n",
       "      <th>개인</th>\n",
       "      <th>기관</th>\n",
       "      <th>외인(수량)</th>\n",
       "      <th>외국계</th>\n",
       "      <th>프로그램</th>\n",
       "      <th>외인비</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021/12/17</td>\n",
       "      <td>76800</td>\n",
       "      <td>78000</td>\n",
       "      <td>76800</td>\n",
       "      <td>78000</td>\n",
       "      <td>▲</td>\n",
       "      <td>200</td>\n",
       "      <td>0.26</td>\n",
       "      <td>11802494.0</td>\n",
       "      <td>914987.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-733323</td>\n",
       "      <td>-907696</td>\n",
       "      <td>0</td>\n",
       "      <td>-257019</td>\n",
       "      <td>757837</td>\n",
       "      <td>51.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021/12/16</td>\n",
       "      <td>78500</td>\n",
       "      <td>78500</td>\n",
       "      <td>77400</td>\n",
       "      <td>77800</td>\n",
       "      <td>▲</td>\n",
       "      <td>200</td>\n",
       "      <td>0.26</td>\n",
       "      <td>11996128.0</td>\n",
       "      <td>934244.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-442445</td>\n",
       "      <td>-261746</td>\n",
       "      <td>-105777</td>\n",
       "      <td>571543</td>\n",
       "      <td>822030</td>\n",
       "      <td>51.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021/12/15</td>\n",
       "      <td>76400</td>\n",
       "      <td>77600</td>\n",
       "      <td>76300</td>\n",
       "      <td>77600</td>\n",
       "      <td>▲</td>\n",
       "      <td>600</td>\n",
       "      <td>0.78</td>\n",
       "      <td>9584939.0</td>\n",
       "      <td>738592.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1118059</td>\n",
       "      <td>-654764</td>\n",
       "      <td>1095947</td>\n",
       "      <td>1946258</td>\n",
       "      <td>1706254</td>\n",
       "      <td>51.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021/12/14</td>\n",
       "      <td>76500</td>\n",
       "      <td>77200</td>\n",
       "      <td>76200</td>\n",
       "      <td>77000</td>\n",
       "      <td>▲</td>\n",
       "      <td>200</td>\n",
       "      <td>0.26</td>\n",
       "      <td>10976660.0</td>\n",
       "      <td>841447.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>198293</td>\n",
       "      <td>-1487295</td>\n",
       "      <td>1005909</td>\n",
       "      <td>804186</td>\n",
       "      <td>-132070</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021/12/13</td>\n",
       "      <td>77200</td>\n",
       "      <td>78300</td>\n",
       "      <td>76500</td>\n",
       "      <td>76800</td>\n",
       "      <td>▼</td>\n",
       "      <td>-100</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>15038750.0</td>\n",
       "      <td>1163285.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-181359</td>\n",
       "      <td>184966</td>\n",
       "      <td>-151301</td>\n",
       "      <td>-1388477</td>\n",
       "      <td>-606534</td>\n",
       "      <td>51.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021/12/10</td>\n",
       "      <td>77400</td>\n",
       "      <td>77600</td>\n",
       "      <td>76800</td>\n",
       "      <td>76900</td>\n",
       "      <td>▼</td>\n",
       "      <td>-1300</td>\n",
       "      <td>-1.66</td>\n",
       "      <td>9155219.0</td>\n",
       "      <td>705966.0</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1797829</td>\n",
       "      <td>-1071153</td>\n",
       "      <td>-728679</td>\n",
       "      <td>-505955</td>\n",
       "      <td>-1714088</td>\n",
       "      <td>51.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021/12/09</td>\n",
       "      <td>77400</td>\n",
       "      <td>78200</td>\n",
       "      <td>77000</td>\n",
       "      <td>78200</td>\n",
       "      <td>▲</td>\n",
       "      <td>800</td>\n",
       "      <td>1.03</td>\n",
       "      <td>21604528.0</td>\n",
       "      <td>1681184.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-1671678</td>\n",
       "      <td>-491078</td>\n",
       "      <td>2768853</td>\n",
       "      <td>2796574</td>\n",
       "      <td>3486578</td>\n",
       "      <td>51.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021/12/08</td>\n",
       "      <td>78300</td>\n",
       "      <td>78600</td>\n",
       "      <td>77100</td>\n",
       "      <td>77400</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21558340.0</td>\n",
       "      <td>1662979.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-2524520</td>\n",
       "      <td>2586682</td>\n",
       "      <td>703379</td>\n",
       "      <td>-941476</td>\n",
       "      <td>48232</td>\n",
       "      <td>51.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021/12/07</td>\n",
       "      <td>76100</td>\n",
       "      <td>77700</td>\n",
       "      <td>75600</td>\n",
       "      <td>77400</td>\n",
       "      <td>▲</td>\n",
       "      <td>1100</td>\n",
       "      <td>1.44</td>\n",
       "      <td>19232453.0</td>\n",
       "      <td>1477888.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-2997907</td>\n",
       "      <td>-547244</td>\n",
       "      <td>4216958</td>\n",
       "      <td>3518986</td>\n",
       "      <td>123462</td>\n",
       "      <td>51.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021/12/06</td>\n",
       "      <td>75100</td>\n",
       "      <td>76700</td>\n",
       "      <td>74900</td>\n",
       "      <td>76300</td>\n",
       "      <td>▲</td>\n",
       "      <td>700</td>\n",
       "      <td>0.93</td>\n",
       "      <td>16391250.0</td>\n",
       "      <td>1245309.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-3062604</td>\n",
       "      <td>576498</td>\n",
       "      <td>2508191</td>\n",
       "      <td>2146417</td>\n",
       "      <td>2070776</td>\n",
       "      <td>51.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021/12/03</td>\n",
       "      <td>75600</td>\n",
       "      <td>76000</td>\n",
       "      <td>74100</td>\n",
       "      <td>75600</td>\n",
       "      <td>▼</td>\n",
       "      <td>-200</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>18330240.0</td>\n",
       "      <td>1375861.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>133267</td>\n",
       "      <td>-157689</td>\n",
       "      <td>-630400</td>\n",
       "      <td>467310</td>\n",
       "      <td>-532651</td>\n",
       "      <td>51.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021/12/02</td>\n",
       "      <td>73900</td>\n",
       "      <td>75800</td>\n",
       "      <td>73800</td>\n",
       "      <td>75800</td>\n",
       "      <td>▲</td>\n",
       "      <td>1400</td>\n",
       "      <td>1.88</td>\n",
       "      <td>23652940.0</td>\n",
       "      <td>1778785.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-6603764</td>\n",
       "      <td>-164462</td>\n",
       "      <td>6327924</td>\n",
       "      <td>5789997</td>\n",
       "      <td>5214948</td>\n",
       "      <td>51.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021/12/01</td>\n",
       "      <td>72000</td>\n",
       "      <td>74800</td>\n",
       "      <td>71600</td>\n",
       "      <td>74400</td>\n",
       "      <td>▲</td>\n",
       "      <td>3100</td>\n",
       "      <td>4.35</td>\n",
       "      <td>21954856.0</td>\n",
       "      <td>1610885.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-6250521</td>\n",
       "      <td>415580</td>\n",
       "      <td>5498391</td>\n",
       "      <td>5601567</td>\n",
       "      <td>3706678</td>\n",
       "      <td>51.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021/11/30</td>\n",
       "      <td>73200</td>\n",
       "      <td>73900</td>\n",
       "      <td>70500</td>\n",
       "      <td>71300</td>\n",
       "      <td>▼</td>\n",
       "      <td>-1000</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>30364841.0</td>\n",
       "      <td>2183678.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1834720</td>\n",
       "      <td>-2841806</td>\n",
       "      <td>1600552</td>\n",
       "      <td>370200</td>\n",
       "      <td>939616</td>\n",
       "      <td>51.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021/11/29</td>\n",
       "      <td>71700</td>\n",
       "      <td>73000</td>\n",
       "      <td>71400</td>\n",
       "      <td>72300</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16682559.0</td>\n",
       "      <td>1206606.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-2395401</td>\n",
       "      <td>3021851</td>\n",
       "      <td>-494165</td>\n",
       "      <td>-566353</td>\n",
       "      <td>-525529</td>\n",
       "      <td>51.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021/11/26</td>\n",
       "      <td>73500</td>\n",
       "      <td>74100</td>\n",
       "      <td>72000</td>\n",
       "      <td>72300</td>\n",
       "      <td>▼</td>\n",
       "      <td>-1400</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>13002242.0</td>\n",
       "      <td>944378.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1825155</td>\n",
       "      <td>-1070320</td>\n",
       "      <td>-1157622</td>\n",
       "      <td>-560900</td>\n",
       "      <td>-1313864</td>\n",
       "      <td>51.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021/11/25</td>\n",
       "      <td>75100</td>\n",
       "      <td>75100</td>\n",
       "      <td>73600</td>\n",
       "      <td>73700</td>\n",
       "      <td>▼</td>\n",
       "      <td>-1100</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>12559258.0</td>\n",
       "      <td>929571.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>694786</td>\n",
       "      <td>-179371</td>\n",
       "      <td>-353300</td>\n",
       "      <td>-143417</td>\n",
       "      <td>-704752</td>\n",
       "      <td>51.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021/11/24</td>\n",
       "      <td>76000</td>\n",
       "      <td>76200</td>\n",
       "      <td>74100</td>\n",
       "      <td>74800</td>\n",
       "      <td>▼</td>\n",
       "      <td>-500</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>15652305.0</td>\n",
       "      <td>1174196.0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>701745</td>\n",
       "      <td>-2258726</td>\n",
       "      <td>1484318</td>\n",
       "      <td>1421283</td>\n",
       "      <td>-596182</td>\n",
       "      <td>51.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021/11/23</td>\n",
       "      <td>76000</td>\n",
       "      <td>76000</td>\n",
       "      <td>74500</td>\n",
       "      <td>75300</td>\n",
       "      <td>▲</td>\n",
       "      <td>400</td>\n",
       "      <td>0.53</td>\n",
       "      <td>22029195.0</td>\n",
       "      <td>1656722.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-2177512</td>\n",
       "      <td>-2811514</td>\n",
       "      <td>4884832</td>\n",
       "      <td>5578065</td>\n",
       "      <td>1247451</td>\n",
       "      <td>51.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021/11/22</td>\n",
       "      <td>73300</td>\n",
       "      <td>75200</td>\n",
       "      <td>73000</td>\n",
       "      <td>74900</td>\n",
       "      <td>▲</td>\n",
       "      <td>3700</td>\n",
       "      <td>5.20</td>\n",
       "      <td>27506623.0</td>\n",
       "      <td>2047228.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-8689803</td>\n",
       "      <td>3034399</td>\n",
       "      <td>5716921</td>\n",
       "      <td>4760374</td>\n",
       "      <td>3500256</td>\n",
       "      <td>51.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            일자     시가     고가     저가     종가 전일비  Unnamed: 6   등락률         거래량  \\\n",
       "0   2021/12/17  76800  78000  76800  78000   ▲         200  0.26  11802494.0   \n",
       "1   2021/12/16  78500  78500  77400  77800   ▲         200  0.26  11996128.0   \n",
       "2   2021/12/15  76400  77600  76300  77600   ▲         600  0.78   9584939.0   \n",
       "3   2021/12/14  76500  77200  76200  77000   ▲         200  0.26  10976660.0   \n",
       "4   2021/12/13  77200  78300  76500  76800   ▼        -100 -0.13  15038750.0   \n",
       "5   2021/12/10  77400  77600  76800  76900   ▼       -1300 -1.66   9155219.0   \n",
       "6   2021/12/09  77400  78200  77000  78200   ▲         800  1.03  21604528.0   \n",
       "7   2021/12/08  78300  78600  77100  77400               0  0.00  21558340.0   \n",
       "8   2021/12/07  76100  77700  75600  77400   ▲        1100  1.44  19232453.0   \n",
       "9   2021/12/06  75100  76700  74900  76300   ▲         700  0.93  16391250.0   \n",
       "10  2021/12/03  75600  76000  74100  75600   ▼        -200 -0.26  18330240.0   \n",
       "11  2021/12/02  73900  75800  73800  75800   ▲        1400  1.88  23652940.0   \n",
       "12  2021/12/01  72000  74800  71600  74400   ▲        3100  4.35  21954856.0   \n",
       "13  2021/11/30  73200  73900  70500  71300   ▼       -1000 -1.38  30364841.0   \n",
       "14  2021/11/29  71700  73000  71400  72300               0  0.00  16682559.0   \n",
       "15  2021/11/26  73500  74100  72000  72300   ▼       -1400 -1.90  13002242.0   \n",
       "16  2021/11/25  75100  75100  73600  73700   ▼       -1100 -1.47  12559258.0   \n",
       "17  2021/11/24  76000  76200  74100  74800   ▼        -500 -0.66  15652305.0   \n",
       "18  2021/11/23  76000  76000  74500  75300   ▲         400  0.53  22029195.0   \n",
       "19  2021/11/22  73300  75200  73000  74900   ▲        3700  5.20  27506623.0   \n",
       "\n",
       "       금액(백만)   신용비       개인       기관   외인(수량)      외국계     프로그램    외인비  \n",
       "0    914987.0  0.00  -733323  -907696        0  -257019   757837  51.78  \n",
       "1    934244.0  0.13  -442445  -261746  -105777   571543   822030  51.78  \n",
       "2    738592.0  0.14 -1118059  -654764  1095947  1946258  1706254  51.79  \n",
       "3    841447.0  0.14   198293 -1487295  1005909   804186  -132070  51.77  \n",
       "4   1163285.0  0.13  -181359   184966  -151301 -1388477  -606534  51.75  \n",
       "5    705966.0  0.13  1797829 -1071153  -728679  -505955 -1714088  51.75  \n",
       "6   1681184.0  0.14 -1671678  -491078  2768853  2796574  3486578  51.77  \n",
       "7   1662979.0  0.14 -2524520  2586682   703379  -941476    48232  51.72  \n",
       "8   1477888.0  0.14 -2997907  -547244  4216958  3518986   123462  51.71  \n",
       "9   1245309.0  0.14 -3062604   576498  2508191  2146417  2070776  51.64  \n",
       "10  1375861.0  0.15   133267  -157689  -630400   467310  -532651  51.60  \n",
       "11  1778785.0  0.15 -6603764  -164462  6327924  5789997  5214948  51.61  \n",
       "12  1610885.0  0.15 -6250521   415580  5498391  5601567  3706678  51.50  \n",
       "13  2183678.0  0.15  1834720 -2841806  1600552   370200   939616  51.41  \n",
       "14  1206606.0  0.15 -2395401  3021851  -494165  -566353  -525529  51.38  \n",
       "15   944378.0  0.15  1825155 -1070320 -1157622  -560900 -1313864  51.39  \n",
       "16   929571.0  0.15   694786  -179371  -353300  -143417  -704752  51.41  \n",
       "17  1174196.0  0.15   701745 -2258726  1484318  1421283  -596182  51.41  \n",
       "18  1656722.0  0.16 -2177512 -2811514  4884832  5578065  1247451  51.39  \n",
       "19  2047228.0  0.16 -8689803  3034399  5716921  4760374  3500256  51.31  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ss = pd.read_csv(\"D:/_data/stock predict/삼성전자.csv\", thousands=',', encoding='CP949')\n",
    "ss = ss.drop(range(20, 1120), axis=0)\n",
    "ki = pd.read_csv(\"D:/_data/stock predict/키움증권.csv\", thousands=',', encoding='CP949')\n",
    "ki = ki.drop(range(20, 1060), axis=0)\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Dropout\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>종가</th>\n",
       "      <th>전일비</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>등락률</th>\n",
       "      <th>거래량</th>\n",
       "      <th>금액(백만)</th>\n",
       "      <th>신용비</th>\n",
       "      <th>개인</th>\n",
       "      <th>기관</th>\n",
       "      <th>외인(수량)</th>\n",
       "      <th>외국계</th>\n",
       "      <th>프로그램</th>\n",
       "      <th>외인비</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021/11/22</td>\n",
       "      <td>106000</td>\n",
       "      <td>109000</td>\n",
       "      <td>105000</td>\n",
       "      <td>109000</td>\n",
       "      <td>▲</td>\n",
       "      <td>3500</td>\n",
       "      <td>3.32</td>\n",
       "      <td>89564</td>\n",
       "      <td>9563</td>\n",
       "      <td>0.93</td>\n",
       "      <td>-13426</td>\n",
       "      <td>-10878</td>\n",
       "      <td>24865</td>\n",
       "      <td>23544</td>\n",
       "      <td>24075</td>\n",
       "      <td>25.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021/11/23</td>\n",
       "      <td>107500</td>\n",
       "      <td>107500</td>\n",
       "      <td>105500</td>\n",
       "      <td>107500</td>\n",
       "      <td>▼</td>\n",
       "      <td>-1500</td>\n",
       "      <td>-1.38</td>\n",
       "      <td>57672</td>\n",
       "      <td>6143</td>\n",
       "      <td>0.94</td>\n",
       "      <td>11818</td>\n",
       "      <td>-13417</td>\n",
       "      <td>-1065</td>\n",
       "      <td>1528</td>\n",
       "      <td>-8687</td>\n",
       "      <td>25.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021/11/24</td>\n",
       "      <td>107000</td>\n",
       "      <td>107000</td>\n",
       "      <td>105000</td>\n",
       "      <td>105000</td>\n",
       "      <td>▼</td>\n",
       "      <td>-2500</td>\n",
       "      <td>-2.33</td>\n",
       "      <td>62724</td>\n",
       "      <td>6620</td>\n",
       "      <td>0.93</td>\n",
       "      <td>26965</td>\n",
       "      <td>-8031</td>\n",
       "      <td>-26006</td>\n",
       "      <td>-17199</td>\n",
       "      <td>-20042</td>\n",
       "      <td>25.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021/11/25</td>\n",
       "      <td>105000</td>\n",
       "      <td>106500</td>\n",
       "      <td>104000</td>\n",
       "      <td>105500</td>\n",
       "      <td>▲</td>\n",
       "      <td>500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>38215</td>\n",
       "      <td>4026</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-7183</td>\n",
       "      <td>3216</td>\n",
       "      <td>10319</td>\n",
       "      <td>1666</td>\n",
       "      <td>-3350</td>\n",
       "      <td>25.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021/11/26</td>\n",
       "      <td>104500</td>\n",
       "      <td>106000</td>\n",
       "      <td>102000</td>\n",
       "      <td>103000</td>\n",
       "      <td>▼</td>\n",
       "      <td>-2500</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>77033</td>\n",
       "      <td>7991</td>\n",
       "      <td>0.96</td>\n",
       "      <td>14505</td>\n",
       "      <td>5517</td>\n",
       "      <td>-26186</td>\n",
       "      <td>-15546</td>\n",
       "      <td>-25613</td>\n",
       "      <td>25.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021/11/29</td>\n",
       "      <td>100000</td>\n",
       "      <td>104000</td>\n",
       "      <td>100000</td>\n",
       "      <td>102500</td>\n",
       "      <td>▼</td>\n",
       "      <td>-500</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>78470</td>\n",
       "      <td>8007</td>\n",
       "      <td>0.95</td>\n",
       "      <td>-21785</td>\n",
       "      <td>9951</td>\n",
       "      <td>10834</td>\n",
       "      <td>1718</td>\n",
       "      <td>-192</td>\n",
       "      <td>25.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021/11/30</td>\n",
       "      <td>102500</td>\n",
       "      <td>103000</td>\n",
       "      <td>96000</td>\n",
       "      <td>97500</td>\n",
       "      <td>▼</td>\n",
       "      <td>-5000</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>169524</td>\n",
       "      <td>16764</td>\n",
       "      <td>0.93</td>\n",
       "      <td>62687</td>\n",
       "      <td>-27749</td>\n",
       "      <td>-59027</td>\n",
       "      <td>-43107</td>\n",
       "      <td>-47586</td>\n",
       "      <td>25.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021/12/01</td>\n",
       "      <td>97000</td>\n",
       "      <td>99800</td>\n",
       "      <td>96600</td>\n",
       "      <td>99600</td>\n",
       "      <td>▲</td>\n",
       "      <td>2100</td>\n",
       "      <td>2.15</td>\n",
       "      <td>79903</td>\n",
       "      <td>7875</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-18847</td>\n",
       "      <td>4984</td>\n",
       "      <td>8628</td>\n",
       "      <td>8973</td>\n",
       "      <td>9911</td>\n",
       "      <td>25.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021/12/02</td>\n",
       "      <td>98100</td>\n",
       "      <td>104000</td>\n",
       "      <td>98100</td>\n",
       "      <td>103500</td>\n",
       "      <td>▲</td>\n",
       "      <td>3900</td>\n",
       "      <td>3.92</td>\n",
       "      <td>75890</td>\n",
       "      <td>7705</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-38201</td>\n",
       "      <td>11298</td>\n",
       "      <td>13384</td>\n",
       "      <td>10192</td>\n",
       "      <td>26511</td>\n",
       "      <td>25.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021/12/03</td>\n",
       "      <td>102500</td>\n",
       "      <td>106000</td>\n",
       "      <td>102000</td>\n",
       "      <td>105500</td>\n",
       "      <td>▲</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.93</td>\n",
       "      <td>65729</td>\n",
       "      <td>6890</td>\n",
       "      <td>0.92</td>\n",
       "      <td>-21284</td>\n",
       "      <td>13087</td>\n",
       "      <td>7616</td>\n",
       "      <td>669</td>\n",
       "      <td>3073</td>\n",
       "      <td>25.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021/12/06</td>\n",
       "      <td>105500</td>\n",
       "      <td>108500</td>\n",
       "      <td>104000</td>\n",
       "      <td>107500</td>\n",
       "      <td>▲</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.90</td>\n",
       "      <td>77250</td>\n",
       "      <td>8276</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-40876</td>\n",
       "      <td>5377</td>\n",
       "      <td>36004</td>\n",
       "      <td>26747</td>\n",
       "      <td>17436</td>\n",
       "      <td>25.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021/12/07</td>\n",
       "      <td>107500</td>\n",
       "      <td>108500</td>\n",
       "      <td>106500</td>\n",
       "      <td>108000</td>\n",
       "      <td>▲</td>\n",
       "      <td>500</td>\n",
       "      <td>0.47</td>\n",
       "      <td>46518</td>\n",
       "      <td>5008</td>\n",
       "      <td>0.87</td>\n",
       "      <td>10363</td>\n",
       "      <td>296</td>\n",
       "      <td>-26729</td>\n",
       "      <td>-5199</td>\n",
       "      <td>-10394</td>\n",
       "      <td>25.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021/12/08</td>\n",
       "      <td>108000</td>\n",
       "      <td>109500</td>\n",
       "      <td>107500</td>\n",
       "      <td>107500</td>\n",
       "      <td>▼</td>\n",
       "      <td>-500</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>49085</td>\n",
       "      <td>5329</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-7038</td>\n",
       "      <td>5396</td>\n",
       "      <td>-4363</td>\n",
       "      <td>1143</td>\n",
       "      <td>-591</td>\n",
       "      <td>25.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021/12/09</td>\n",
       "      <td>108500</td>\n",
       "      <td>109000</td>\n",
       "      <td>106500</td>\n",
       "      <td>108000</td>\n",
       "      <td>▲</td>\n",
       "      <td>500</td>\n",
       "      <td>0.47</td>\n",
       "      <td>62923</td>\n",
       "      <td>6788</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-3385</td>\n",
       "      <td>7495</td>\n",
       "      <td>6090</td>\n",
       "      <td>970</td>\n",
       "      <td>1767</td>\n",
       "      <td>25.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021/12/10</td>\n",
       "      <td>108000</td>\n",
       "      <td>108500</td>\n",
       "      <td>106500</td>\n",
       "      <td>106500</td>\n",
       "      <td>▼</td>\n",
       "      <td>-1500</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>28979</td>\n",
       "      <td>3105</td>\n",
       "      <td>0.85</td>\n",
       "      <td>2080</td>\n",
       "      <td>-6921</td>\n",
       "      <td>8941</td>\n",
       "      <td>-798</td>\n",
       "      <td>-1877</td>\n",
       "      <td>25.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021/12/13</td>\n",
       "      <td>107000</td>\n",
       "      <td>109500</td>\n",
       "      <td>107000</td>\n",
       "      <td>107500</td>\n",
       "      <td>▲</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.94</td>\n",
       "      <td>52503</td>\n",
       "      <td>5695</td>\n",
       "      <td>0.83</td>\n",
       "      <td>-13963</td>\n",
       "      <td>7746</td>\n",
       "      <td>7576</td>\n",
       "      <td>695</td>\n",
       "      <td>8305</td>\n",
       "      <td>25.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021/12/14</td>\n",
       "      <td>106500</td>\n",
       "      <td>109000</td>\n",
       "      <td>106500</td>\n",
       "      <td>107000</td>\n",
       "      <td>▼</td>\n",
       "      <td>-500</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>64977</td>\n",
       "      <td>6974</td>\n",
       "      <td>0.83</td>\n",
       "      <td>4777</td>\n",
       "      <td>-20135</td>\n",
       "      <td>15033</td>\n",
       "      <td>-1107</td>\n",
       "      <td>2477</td>\n",
       "      <td>25.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021/12/15</td>\n",
       "      <td>107000</td>\n",
       "      <td>108000</td>\n",
       "      <td>106500</td>\n",
       "      <td>107500</td>\n",
       "      <td>▲</td>\n",
       "      <td>500</td>\n",
       "      <td>0.47</td>\n",
       "      <td>23210</td>\n",
       "      <td>2496</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-4089</td>\n",
       "      <td>1368</td>\n",
       "      <td>-915</td>\n",
       "      <td>5065</td>\n",
       "      <td>-1671</td>\n",
       "      <td>25.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021/12/16</td>\n",
       "      <td>109500</td>\n",
       "      <td>109500</td>\n",
       "      <td>107000</td>\n",
       "      <td>107500</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45031</td>\n",
       "      <td>4858</td>\n",
       "      <td>0.82</td>\n",
       "      <td>956</td>\n",
       "      <td>13405</td>\n",
       "      <td>-23661</td>\n",
       "      <td>-10182</td>\n",
       "      <td>-12948</td>\n",
       "      <td>25.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021/12/17</td>\n",
       "      <td>107000</td>\n",
       "      <td>109500</td>\n",
       "      <td>106500</td>\n",
       "      <td>109500</td>\n",
       "      <td>▲</td>\n",
       "      <td>2000</td>\n",
       "      <td>1.86</td>\n",
       "      <td>60487</td>\n",
       "      <td>6576</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-16202</td>\n",
       "      <td>6443</td>\n",
       "      <td>0</td>\n",
       "      <td>7348</td>\n",
       "      <td>8805</td>\n",
       "      <td>25.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            일자      시가      고가      저가      종가 전일비  Unnamed: 6   등락률     거래량  \\\n",
       "0   2021/11/22  106000  109000  105000  109000   ▲        3500  3.32   89564   \n",
       "1   2021/11/23  107500  107500  105500  107500   ▼       -1500 -1.38   57672   \n",
       "2   2021/11/24  107000  107000  105000  105000   ▼       -2500 -2.33   62724   \n",
       "3   2021/11/25  105000  106500  104000  105500   ▲         500  0.48   38215   \n",
       "4   2021/11/26  104500  106000  102000  103000   ▼       -2500 -2.37   77033   \n",
       "5   2021/11/29  100000  104000  100000  102500   ▼        -500 -0.49   78470   \n",
       "6   2021/11/30  102500  103000   96000   97500   ▼       -5000 -4.88  169524   \n",
       "7   2021/12/01   97000   99800   96600   99600   ▲        2100  2.15   79903   \n",
       "8   2021/12/02   98100  104000   98100  103500   ▲        3900  3.92   75890   \n",
       "9   2021/12/03  102500  106000  102000  105500   ▲        2000  1.93   65729   \n",
       "10  2021/12/06  105500  108500  104000  107500   ▲        2000  1.90   77250   \n",
       "11  2021/12/07  107500  108500  106500  108000   ▲         500  0.47   46518   \n",
       "12  2021/12/08  108000  109500  107500  107500   ▼        -500 -0.46   49085   \n",
       "13  2021/12/09  108500  109000  106500  108000   ▲         500  0.47   62923   \n",
       "14  2021/12/10  108000  108500  106500  106500   ▼       -1500 -1.39   28979   \n",
       "15  2021/12/13  107000  109500  107000  107500   ▲        1000  0.94   52503   \n",
       "16  2021/12/14  106500  109000  106500  107000   ▼        -500 -0.47   64977   \n",
       "17  2021/12/15  107000  108000  106500  107500   ▲         500  0.47   23210   \n",
       "18  2021/12/16  109500  109500  107000  107500               0  0.00   45031   \n",
       "19  2021/12/17  107000  109500  106500  109500   ▲        2000  1.86   60487   \n",
       "\n",
       "    금액(백만)   신용비     개인     기관  외인(수량)    외국계   프로그램    외인비  \n",
       "0     9563  0.93 -13426 -10878   24865  23544  24075  25.61  \n",
       "1     6143  0.94  11818 -13417   -1065   1528  -8687  25.61  \n",
       "2     6620  0.93  26965  -8031  -26006 -17199 -20042  25.51  \n",
       "3     4026  0.94  -7183   3216   10319   1666  -3350  25.55  \n",
       "4     7991  0.96  14505   5517  -26186 -15546 -25613  25.45  \n",
       "5     8007  0.95 -21785   9951   10834   1718   -192  25.49  \n",
       "6    16764  0.93  62687 -27749  -59027 -43107 -47586  25.27  \n",
       "7     7875  0.91 -18847   4984    8628   8973   9911  25.30  \n",
       "8     7705  0.94 -38201  11298   13384  10192  26511  25.35  \n",
       "9     6890  0.92 -21284  13087    7616    669   3073  25.38  \n",
       "10    8276  0.89 -40876   5377   36004  26747  17436  25.52  \n",
       "11    5008  0.87  10363    296  -26729  -5199 -10394  25.41  \n",
       "12    5329  0.85  -7038   5396   -4363   1143   -591  25.40  \n",
       "13    6788  0.86  -3385   7495    6090    970   1767  25.42  \n",
       "14    3105  0.85   2080  -6921    8941   -798  -1877  25.46  \n",
       "15    5695  0.83 -13963   7746    7576    695   8305  25.48  \n",
       "16    6974  0.83   4777 -20135   15033  -1107   2477  25.54  \n",
       "17    2496  0.81  -4089   1368    -915   5065  -1671  25.54  \n",
       "18    4858  0.82    956  13405  -23661 -10182 -12948  25.45  \n",
       "19    6576  0.00 -16202   6443       0   7348   8805  25.45  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 재배열\n",
    "ss = ss.loc[::-1].reset_index(drop=True)\n",
    "ki = ki.loc[::-1].reset_index(drop=True)\n",
    "\n",
    "ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 2)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 컬럼만 남겨두기\n",
    "x_ss = ss.drop(['일자', '시가', '고가', '저가', '종가', '전일비', 'Unnamed: 6', '등락률',\n",
    "                '신용비', '개인', '기관', '외인(수량)', '외국계', '프로그램', '외인비'], axis =1)\n",
    "x_ss = np.array(x_ss)\n",
    "x_ki = ki.drop(['일자', '시가', '고가', '저가', '종가', '전일비', 'Unnamed: 6', '등락률',\n",
    "                '신용비',  '개인', '기관', '외인(수량)', '외국계', '프로그램', '외인비'], axis =1)\n",
    "x_ki = np.array(x_ki)\n",
    "\n",
    "\n",
    "# '일자', '시가', '고가', '저가', '종가', '전일비', 'Unnamed: 6', '등락률', '거래량', '금액(백만)', '신용비', '개인', '기관', '외인(수량)', '외국계', '프로그램', '외인비'\n",
    "\n",
    "x_ss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17, 2, 1), (17, 3))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split 함수 정의\n",
    "def split_xy(dataset, time_steps, y_column):\n",
    "    x, y = list(), list()\n",
    "    for i in range(len(dataset)):\n",
    "        x_end_number = i + time_steps\n",
    "        y_end_number = x_end_number + y_column-1\n",
    "        \n",
    "        if y_end_number > len(dataset):\n",
    "            break\n",
    "        tmp_x = dataset[i:x_end_number, 1:]\n",
    "        tmp_y = dataset[x_end_number-1:y_end_number, 0]\n",
    "        x.append(tmp_x)\n",
    "        y.append(tmp_y)\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x_ssp, y_ssp = split_xy(x_ss, 2, 3)\n",
    "x_kip, y_kip = split_xy(x_ki, 2, 3)\n",
    "\n",
    "x_ssp.shape, y_ssp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삼성 데이터 train_test_split 적용\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x_ssp, y_ssp, train_size=0.8, random_state=66)\n",
    "\n",
    "# 키움 데이터 train_test_split 적용\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x_kip, y_kip, train_size=0.8, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#삼성 input\n",
    "input1 = Input(shape=(2, 1))\n",
    "dense1_1 = LSTM(32, activation='relu')(input1)\n",
    "dense1_2 = Dense(16, activation='relu')(dense1_1)\n",
    "output1 = Dense(4, activation='relu')(dense1_2)\n",
    "\n",
    "#키움 input\n",
    "input2 = Input(shape=(2, 1))\n",
    "dense2_1 = LSTM(32, activation='relu')(input2)\n",
    "dense2_2 = Dense(16, activation='relu')(dense2_1)\n",
    "output2 = Dense(4, activation='relu')(dense2_2)\n",
    "\n",
    "#앙상블\n",
    "from tensorflow.keras.layers import concatenate\n",
    "merge1 = concatenate([output1, output2])\n",
    "\n",
    "#삼성 out\n",
    "output1_1 = Dense(16, activation='relu')(merge1)\n",
    "output1_2 = Dense(8)(output1_1)\n",
    "ss_output = Dense(3)(output1_2)\n",
    "\n",
    "#키움 out\n",
    "output2_1 = Dense(16, activation='relu')(merge1)\n",
    "output2_2 = Dense(8)(output2_1)\n",
    "ku_output = Dense(3)(output2_2)\n",
    "\n",
    "model = Model(inputs=[input1, input2], outputs=[ss_output, ku_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 2, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           [(None, 2, 1)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  (None, 32)           4352        input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 32)           4352        input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_100 (Dense)               (None, 16)           528         lstm_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 16)           528         lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 4)            68          dense_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 4)            68          dense_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 8)            0           dense_101[0][0]                  \n",
      "                                                                 dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 16)           144         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_107 (Dense)               (None, 16)           144         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 8)            136         dense_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 8)            136         dense_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (None, 3)            27          dense_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_109 (Dense)               (None, 3)            27          dense_108[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,510\n",
      "Trainable params: 10,510\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴파일 & 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1/9 [==>...........................] - ETA: 10s - loss: 19520590.0000 - dense_106_loss: 19450582.0000 - dense_109_loss: 70007.6953WARNING:tensorflow:5 out of the last 252 calls to <function Model.make_test_function.<locals>.test_function at 0x0000028024FA4040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "9/9 [==============================] - 2s 32ms/step - loss: 17600424.0000 - dense_106_loss: 17540160.0000 - dense_109_loss: 60266.1602 - val_loss: 17214514.0000 - val_dense_106_loss: 17158522.0000 - val_dense_109_loss: 55990.9883\n",
      "Epoch 2/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17581422.0000 - dense_106_loss: 17528234.0000 - dense_109_loss: 53191.3750 - val_loss: 17188072.0000 - val_dense_106_loss: 17141168.0000 - val_dense_109_loss: 46905.1289\n",
      "Epoch 3/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17553506.0000 - dense_106_loss: 17511334.0000 - dense_109_loss: 42171.9141 - val_loss: 17155030.0000 - val_dense_106_loss: 17121470.0000 - val_dense_109_loss: 33558.7266\n",
      "Epoch 4/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17515878.0000 - dense_106_loss: 17485930.0000 - dense_109_loss: 29947.2051 - val_loss: 17111176.0000 - val_dense_106_loss: 17092456.0000 - val_dense_109_loss: 18719.6406\n",
      "Epoch 5/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17473478.0000 - dense_106_loss: 17448214.0000 - dense_109_loss: 25264.2422 - val_loss: 17072982.0000 - val_dense_106_loss: 17050290.0000 - val_dense_109_loss: 22693.1621\n",
      "Epoch 6/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17426286.0000 - dense_106_loss: 17396366.0000 - dense_109_loss: 29920.2500 - val_loss: 17019304.0000 - val_dense_106_loss: 16991950.0000 - val_dense_109_loss: 27353.2129\n",
      "Epoch 7/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17348412.0000 - dense_106_loss: 17312410.0000 - dense_109_loss: 36003.6758 - val_loss: 16921142.0000 - val_dense_106_loss: 16892416.0000 - val_dense_109_loss: 28725.4082\n",
      "Epoch 8/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 17214652.0000 - dense_106_loss: 17178264.0000 - dense_109_loss: 36388.1328 - val_loss: 16763619.0000 - val_dense_106_loss: 16734487.0000 - val_dense_109_loss: 29133.7031\n",
      "Epoch 9/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 16973198.0000 - dense_106_loss: 16938810.0000 - dense_109_loss: 34389.3789 - val_loss: 16518550.0000 - val_dense_106_loss: 16491390.0000 - val_dense_109_loss: 27160.5215\n",
      "Epoch 10/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 16566660.0000 - dense_106_loss: 16535396.0000 - dense_109_loss: 31263.6328 - val_loss: 15985330.0000 - val_dense_106_loss: 15945142.0000 - val_dense_109_loss: 40187.1133\n",
      "Epoch 11/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 16018923.0000 - dense_106_loss: 15989218.0000 - dense_109_loss: 29704.7500 - val_loss: 15292599.0000 - val_dense_106_loss: 15256603.0000 - val_dense_109_loss: 35996.3086\n",
      "Epoch 12/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 15102782.0000 - dense_106_loss: 15069388.0000 - dense_109_loss: 33394.7695 - val_loss: 14224263.0000 - val_dense_106_loss: 14192983.0000 - val_dense_109_loss: 31280.0332\n",
      "Epoch 13/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 13647335.0000 - dense_106_loss: 13622405.0000 - dense_109_loss: 24930.0527 - val_loss: 12639178.0000 - val_dense_106_loss: 12604514.0000 - val_dense_109_loss: 34662.5234\n",
      "Epoch 14/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 11413304.0000 - dense_106_loss: 11384508.0000 - dense_109_loss: 28795.8047 - val_loss: 9728784.0000 - val_dense_106_loss: 9709220.0000 - val_dense_109_loss: 19565.4824\n",
      "Epoch 15/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 10828586.0000 - dense_106_loss: 10813013.0000 - dense_109_loss: 15573.323 - 0s 3ms/step - loss: 8894551.0000 - dense_106_loss: 8868454.0000 - dense_109_loss: 26096.8301 - val_loss: 7189154.0000 - val_dense_106_loss: 7150033.0000 - val_dense_109_loss: 39120.2695\n",
      "Epoch 16/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6942518.0000 - dense_106_loss: 6917176.5000 - dense_109_loss: 25342.3711 - val_loss: 6006251.5000 - val_dense_106_loss: 5986581.0000 - val_dense_109_loss: 19670.4238\n",
      "Epoch 17/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6447932.5000 - dense_106_loss: 6423973.5000 - dense_109_loss: 23959.6445 - val_loss: 5540336.0000 - val_dense_106_loss: 5515201.5000 - val_dense_109_loss: 25134.6445\n",
      "Epoch 18/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 6128062.0000 - dense_106_loss: 6101242.5000 - dense_109_loss: 26818.7285 - val_loss: 5646625.0000 - val_dense_106_loss: 5616960.5000 - val_dense_109_loss: 29664.5352\n",
      "Epoch 19/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 5754388.5000 - dense_106_loss: 5730088.5000 - dense_109_loss: 24299.2285 - val_loss: 5436477.0000 - val_dense_106_loss: 5412262.0000 - val_dense_109_loss: 24215.3242\n",
      "Epoch 20/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 5354957.0000 - dense_106_loss: 5332238.0000 - dense_109_loss: 22719.2969 - val_loss: 5218670.0000 - val_dense_106_loss: 5193153.5000 - val_dense_109_loss: 25516.2500\n",
      "Epoch 21/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 4954146.0000 - dense_106_loss: 4931790.0000 - dense_109_loss: 22356.1328 - val_loss: 5037562.0000 - val_dense_106_loss: 5018155.0000 - val_dense_109_loss: 19406.5410\n",
      "Epoch 22/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 4604092.5000 - dense_106_loss: 4581896.5000 - dense_109_loss: 22195.5332 - val_loss: 5017109.0000 - val_dense_106_loss: 4983224.0000 - val_dense_109_loss: 33885.0859\n",
      "Epoch 23/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 4267664.5000 - dense_106_loss: 4237925.5000 - dense_109_loss: 29738.8262 - val_loss: 4946335.5000 - val_dense_106_loss: 4923363.0000 - val_dense_109_loss: 22972.4863\n",
      "Epoch 24/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3918895.0000 - dense_106_loss: 3897113.0000 - dense_109_loss: 21781.9238 - val_loss: 4944516.5000 - val_dense_106_loss: 4902988.5000 - val_dense_109_loss: 41528.0859\n",
      "Epoch 25/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3662527.2500 - dense_106_loss: 3635772.7500 - dense_109_loss: 26754.8477 - val_loss: 4885795.0000 - val_dense_106_loss: 4865799.0000 - val_dense_109_loss: 19996.2109\n",
      "Epoch 26/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3577050.5000 - dense_106_loss: 3546096.5000 - dense_109_loss: 30953.9961 - val_loss: 4872074.0000 - val_dense_106_loss: 4850032.5000 - val_dense_109_loss: 22041.7129\n",
      "Epoch 27/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3420021.5000 - dense_106_loss: 3393886.2500 - dense_109_loss: 26135.2773 - val_loss: 4867994.0000 - val_dense_106_loss: 4835237.5000 - val_dense_109_loss: 32756.8809\n",
      "Epoch 28/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3446043.7500 - dense_106_loss: 3419233.0000 - dense_109_loss: 26811.5840 - val_loss: 4831578.5000 - val_dense_106_loss: 4802380.0000 - val_dense_109_loss: 29198.3418\n",
      "Epoch 29/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3383540.0000 - dense_106_loss: 3356112.2500 - dense_109_loss: 27427.5254 - val_loss: 4505188.0000 - val_dense_106_loss: 4483184.0000 - val_dense_109_loss: 22004.1445\n",
      "Epoch 30/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3397956.5000 - dense_106_loss: 3366385.0000 - dense_109_loss: 31571.7598 - val_loss: 4549655.5000 - val_dense_106_loss: 4523937.5000 - val_dense_109_loss: 25718.1230\n",
      "Epoch 31/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3331339.7500 - dense_106_loss: 3302639.0000 - dense_109_loss: 28700.7051 - val_loss: 4766593.0000 - val_dense_106_loss: 4744278.5000 - val_dense_109_loss: 22314.3770\n",
      "Epoch 32/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3370936.2500 - dense_106_loss: 3341958.2500 - dense_109_loss: 28978.1445 - val_loss: 4741082.0000 - val_dense_106_loss: 4720628.5000 - val_dense_109_loss: 20453.6758\n",
      "Epoch 33/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3324101.0000 - dense_106_loss: 3291281.7500 - dense_109_loss: 32818.8594 - val_loss: 4519350.0000 - val_dense_106_loss: 4477083.0000 - val_dense_109_loss: 42266.4336\n",
      "Epoch 34/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3280011.5000 - dense_106_loss: 3254874.2500 - dense_109_loss: 25137.3164 - val_loss: 4481570.5000 - val_dense_106_loss: 4461224.0000 - val_dense_109_loss: 20346.0977\n",
      "Epoch 35/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3229371.7500 - dense_106_loss: 3204547.0000 - dense_109_loss: 24824.5859 - val_loss: 4655827.5000 - val_dense_106_loss: 4635384.5000 - val_dense_109_loss: 20442.9531\n",
      "Epoch 36/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3294447.5000 - dense_106_loss: 3272425.0000 - dense_109_loss: 22022.0625 - val_loss: 4637075.5000 - val_dense_106_loss: 4608854.5000 - val_dense_109_loss: 28221.2051\n",
      "Epoch 37/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3215468.0000 - dense_106_loss: 3191177.2500 - dense_109_loss: 24290.5645 - val_loss: 4384169.5000 - val_dense_106_loss: 4362846.0000 - val_dense_109_loss: 21323.0195\n",
      "Epoch 38/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3196287.0000 - dense_106_loss: 3172097.0000 - dense_109_loss: 24189.9453 - val_loss: 4380795.5000 - val_dense_106_loss: 4353017.5000 - val_dense_109_loss: 27778.0039\n",
      "Epoch 39/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3183196.0000 - dense_106_loss: 3159483.0000 - dense_109_loss: 23713.1309 - val_loss: 4502920.0000 - val_dense_106_loss: 4481430.5000 - val_dense_109_loss: 21489.4531\n",
      "Epoch 40/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3187284.5000 - dense_106_loss: 3165135.2500 - dense_109_loss: 22149.0391 - val_loss: 4459032.0000 - val_dense_106_loss: 4431280.0000 - val_dense_109_loss: 27751.8848\n",
      "Epoch 41/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3177753.2500 - dense_106_loss: 3153821.0000 - dense_109_loss: 23932.2363 - val_loss: 4432529.0000 - val_dense_106_loss: 4411044.0000 - val_dense_109_loss: 21485.3750\n",
      "Epoch 42/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3147163.5000 - dense_106_loss: 3125336.0000 - dense_109_loss: 21827.6680 - val_loss: 4390866.0000 - val_dense_106_loss: 4367884.5000 - val_dense_109_loss: 22981.3223\n",
      "Epoch 43/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3161331.7500 - dense_106_loss: 3133872.5000 - dense_109_loss: 27459.2773 - val_loss: 4388117.0000 - val_dense_106_loss: 4367762.5000 - val_dense_109_loss: 20354.8828\n",
      "Epoch 44/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3095530.7500 - dense_106_loss: 3073458.0000 - dense_109_loss: 22073.1836 - val_loss: 4338118.0000 - val_dense_106_loss: 4319873.0000 - val_dense_109_loss: 18245.1582\n",
      "Epoch 45/1000\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 3118173.7500 - dense_106_loss: 3097957.7500 - dense_109_loss: 20215.7734 - val_loss: 4338600.0000 - val_dense_106_loss: 4315912.0000 - val_dense_109_loss: 22687.7188\n",
      "Epoch 46/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3100677.0000 - dense_106_loss: 3078520.5000 - dense_109_loss: 22156.5078 - val_loss: 4332197.5000 - val_dense_106_loss: 4310476.0000 - val_dense_109_loss: 21721.6543\n",
      "Epoch 47/1000\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 3093085.0000 - dense_106_loss: 3070919.5000 - dense_109_loss: 22165.3320 - val_loss: 4319260.0000 - val_dense_106_loss: 4299043.0000 - val_dense_109_loss: 20216.6621\n",
      "Epoch 48/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3085890.7500 - dense_106_loss: 3063416.0000 - dense_109_loss: 22474.3359 - val_loss: 4325979.0000 - val_dense_106_loss: 4304481.5000 - val_dense_109_loss: 21497.4102\n",
      "Epoch 49/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3130244.2500 - dense_106_loss: 3107344.5000 - dense_109_loss: 22899.9141 - val_loss: 4372341.0000 - val_dense_106_loss: 4351213.0000 - val_dense_109_loss: 21127.6191\n",
      "Epoch 50/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3092855.5000 - dense_106_loss: 3071948.7500 - dense_109_loss: 20907.1348 - val_loss: 4310955.0000 - val_dense_106_loss: 4289466.5000 - val_dense_109_loss: 21488.0566\n",
      "Epoch 51/1000\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 3119067.0000 - dense_106_loss: 3097729.7500 - dense_109_loss: 21336.9277 - val_loss: 4333000.5000 - val_dense_106_loss: 4309603.0000 - val_dense_109_loss: 23397.1426\n",
      "Epoch 52/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3043640.7500 - dense_106_loss: 3022633.2500 - dense_109_loss: 21007.0820 - val_loss: 4338615.0000 - val_dense_106_loss: 4318037.5000 - val_dense_109_loss: 20577.0156\n",
      "Epoch 53/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3117119.5000 - dense_106_loss: 3097890.2500 - dense_109_loss: 19229.5156 - val_loss: 4350590.0000 - val_dense_106_loss: 4327441.5000 - val_dense_109_loss: 23148.0078\n",
      "Epoch 54/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3046125.0000 - dense_106_loss: 3022925.0000 - dense_109_loss: 23200.0410 - val_loss: 4321296.5000 - val_dense_106_loss: 4302394.0000 - val_dense_109_loss: 18902.1504\n",
      "Epoch 55/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3104240.7500 - dense_106_loss: 3083097.7500 - dense_109_loss: 21142.4297 - val_loss: 4303851.0000 - val_dense_106_loss: 4284445.5000 - val_dense_109_loss: 19405.0527\n",
      "Epoch 56/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3054590.7500 - dense_106_loss: 3031776.5000 - dense_109_loss: 22814.0840 - val_loss: 4348333.0000 - val_dense_106_loss: 4322247.5000 - val_dense_109_loss: 26085.5879\n",
      "Epoch 57/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3116475.0000 - dense_106_loss: 3095198.7500 - dense_109_loss: 21276.3516 - val_loss: 4355103.5000 - val_dense_106_loss: 4335302.0000 - val_dense_109_loss: 19801.7090\n",
      "Epoch 58/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3113672.0000 - dense_106_loss: 3093902.2500 - dense_109_loss: 19769.6758 - val_loss: 4263885.0000 - val_dense_106_loss: 4240955.0000 - val_dense_109_loss: 22929.5820\n",
      "Epoch 59/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3177579.0000 - dense_106_loss: 3157101.5000 - dense_109_loss: 20477.5508 - val_loss: 4278039.0000 - val_dense_106_loss: 4257530.5000 - val_dense_109_loss: 20508.3809\n",
      "Epoch 60/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3049658.2500 - dense_106_loss: 3022351.0000 - dense_109_loss: 27307.3047 - val_loss: 4320331.5000 - val_dense_106_loss: 4300138.0000 - val_dense_109_loss: 20193.4668\n",
      "Epoch 61/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3073077.5000 - dense_106_loss: 3048850.5000 - dense_109_loss: 24226.9062 - val_loss: 4318347.0000 - val_dense_106_loss: 4294782.5000 - val_dense_109_loss: 23564.3965\n",
      "Epoch 62/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3077270.7500 - dense_106_loss: 3054075.5000 - dense_109_loss: 23195.6328 - val_loss: 4270075.0000 - val_dense_106_loss: 4249099.0000 - val_dense_109_loss: 20975.5039\n",
      "Epoch 63/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3061691.2500 - dense_106_loss: 3039922.5000 - dense_109_loss: 21768.9609 - val_loss: 4293617.5000 - val_dense_106_loss: 4272431.0000 - val_dense_109_loss: 21186.6758\n",
      "Epoch 64/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3040743.5000 - dense_106_loss: 3019578.5000 - dense_109_loss: 21165.2891 - val_loss: 4298506.0000 - val_dense_106_loss: 4279130.0000 - val_dense_109_loss: 19375.6191\n",
      "Epoch 65/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3074466.5000 - dense_106_loss: 3050259.0000 - dense_109_loss: 24207.5391 - val_loss: 4277958.0000 - val_dense_106_loss: 4251737.0000 - val_dense_109_loss: 26221.0176\n",
      "Epoch 66/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3058449.5000 - dense_106_loss: 3036954.0000 - dense_109_loss: 21495.9746 - val_loss: 4269498.0000 - val_dense_106_loss: 4243825.5000 - val_dense_109_loss: 25672.8516\n",
      "Epoch 67/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3037193.7500 - dense_106_loss: 3013431.0000 - dense_109_loss: 23762.6816 - val_loss: 4272919.0000 - val_dense_106_loss: 4248545.0000 - val_dense_109_loss: 24373.6426\n",
      "Epoch 68/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3020089.0000 - dense_106_loss: 2999606.7500 - dense_109_loss: 20482.3945 - val_loss: 4261837.0000 - val_dense_106_loss: 4243251.5000 - val_dense_109_loss: 18585.8477\n",
      "Epoch 69/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3064221.2500 - dense_106_loss: 3043215.7500 - dense_109_loss: 21005.5938 - val_loss: 4230239.0000 - val_dense_106_loss: 4202183.0000 - val_dense_109_loss: 28055.8730\n",
      "Epoch 70/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3066721.5000 - dense_106_loss: 3041491.0000 - dense_109_loss: 25230.4746 - val_loss: 4266058.0000 - val_dense_106_loss: 4243359.5000 - val_dense_109_loss: 22698.7617\n",
      "Epoch 71/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3023939.7500 - dense_106_loss: 3002374.5000 - dense_109_loss: 21565.5762 - val_loss: 4232627.5000 - val_dense_106_loss: 4208460.0000 - val_dense_109_loss: 24167.2734\n",
      "Epoch 72/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3027015.5000 - dense_106_loss: 3005198.2500 - dense_109_loss: 21817.1719 - val_loss: 4236736.0000 - val_dense_106_loss: 4215222.5000 - val_dense_109_loss: 21513.5098\n",
      "Epoch 73/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3079067.0000 - dense_106_loss: 3059201.2500 - dense_109_loss: 19865.5391 - val_loss: 4199018.0000 - val_dense_106_loss: 4175158.0000 - val_dense_109_loss: 23859.7051\n",
      "Epoch 74/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3023866.5000 - dense_106_loss: 3003613.0000 - dense_109_loss: 20253.1211 - val_loss: 4231772.0000 - val_dense_106_loss: 4209110.0000 - val_dense_109_loss: 22661.7910\n",
      "Epoch 75/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3021909.7500 - dense_106_loss: 3001751.7500 - dense_109_loss: 20158.0391 - val_loss: 4202074.0000 - val_dense_106_loss: 4181949.5000 - val_dense_109_loss: 20124.4375\n",
      "Epoch 76/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3022189.5000 - dense_106_loss: 3001703.2500 - dense_109_loss: 20486.0781 - val_loss: 4196965.5000 - val_dense_106_loss: 4177799.0000 - val_dense_109_loss: 19166.5352\n",
      "Epoch 77/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3034399.7500 - dense_106_loss: 3013834.5000 - dense_109_loss: 20565.2949 - val_loss: 4233561.5000 - val_dense_106_loss: 4212374.0000 - val_dense_109_loss: 21187.5977\n",
      "Epoch 78/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3039628.5000 - dense_106_loss: 3019948.2500 - dense_109_loss: 19680.1641 - val_loss: 3888816.5000 - val_dense_106_loss: 3867960.5000 - val_dense_109_loss: 20856.2812\n",
      "Epoch 79/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3033168.0000 - dense_106_loss: 3012453.5000 - dense_109_loss: 20714.3809 - val_loss: 3890013.7500 - val_dense_106_loss: 3870527.5000 - val_dense_109_loss: 19486.2773\n",
      "Epoch 80/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3075282.0000 - dense_106_loss: 3054758.7500 - dense_109_loss: 20522.8984 - val_loss: 3033936.5000 - val_dense_106_loss: 3011777.7500 - val_dense_109_loss: 22158.7910\n",
      "Epoch 81/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3029577.0000 - dense_106_loss: 3008305.5000 - dense_109_loss: 21271.6465 - val_loss: 3045652.5000 - val_dense_106_loss: 3026182.5000 - val_dense_109_loss: 19470.0996\n",
      "Epoch 82/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3030763.0000 - dense_106_loss: 3007457.0000 - dense_109_loss: 23306.3457 - val_loss: 3015668.2500 - val_dense_106_loss: 2991737.0000 - val_dense_109_loss: 23931.3457\n",
      "Epoch 83/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3031266.5000 - dense_106_loss: 3008628.0000 - dense_109_loss: 22638.6387 - val_loss: 3025198.5000 - val_dense_106_loss: 3000241.2500 - val_dense_109_loss: 24957.3633\n",
      "Epoch 84/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3019941.5000 - dense_106_loss: 2995971.0000 - dense_109_loss: 23970.4922 - val_loss: 3027647.5000 - val_dense_106_loss: 3007703.2500 - val_dense_109_loss: 19944.2930\n",
      "Epoch 85/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3043938.2500 - dense_106_loss: 3023170.2500 - dense_109_loss: 20767.7715 - val_loss: 3076811.2500 - val_dense_106_loss: 3049362.2500 - val_dense_109_loss: 27449.3301\n",
      "Epoch 86/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3055243.0000 - dense_106_loss: 3032173.7500 - dense_109_loss: 23068.8672 - val_loss: 3055495.7500 - val_dense_106_loss: 3036150.7500 - val_dense_109_loss: 19344.9512\n",
      "Epoch 87/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3020370.7500 - dense_106_loss: 2999031.0000 - dense_109_loss: 21339.6953 - val_loss: 3009229.2500 - val_dense_106_loss: 2986423.2500 - val_dense_109_loss: 22806.1641\n",
      "Epoch 88/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3039059.0000 - dense_106_loss: 3016856.5000 - dense_109_loss: 22202.5312 - val_loss: 3033446.2500 - val_dense_106_loss: 3013347.2500 - val_dense_109_loss: 20099.2051\n",
      "Epoch 89/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3042313.0000 - dense_106_loss: 3021620.7500 - dense_109_loss: 20692.4238 - val_loss: 3051406.7500 - val_dense_106_loss: 3029628.5000 - val_dense_109_loss: 21778.2578\n",
      "Epoch 90/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 756287.8125 - dense_106_loss: 754582.6875 - dense_109_loss: 1705.09 - 0s 3ms/step - loss: 3074865.0000 - dense_106_loss: 3054362.5000 - dense_109_loss: 20502.1484 - val_loss: 2977049.2500 - val_dense_106_loss: 2957972.7500 - val_dense_109_loss: 19076.3145\n",
      "Epoch 91/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3020267.0000 - dense_106_loss: 2999783.0000 - dense_109_loss: 20483.9453 - val_loss: 3013142.7500 - val_dense_106_loss: 2984834.7500 - val_dense_109_loss: 28307.9688\n",
      "Epoch 92/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3058933.5000 - dense_106_loss: 3037649.2500 - dense_109_loss: 21284.3066 - val_loss: 3042861.5000 - val_dense_106_loss: 3020814.2500 - val_dense_109_loss: 22047.2266\n",
      "Epoch 93/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3029080.5000 - dense_106_loss: 3008286.7500 - dense_109_loss: 20793.8789 - val_loss: 2974676.7500 - val_dense_106_loss: 2951590.2500 - val_dense_109_loss: 23086.4082\n",
      "Epoch 94/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3042311.0000 - dense_106_loss: 3021653.5000 - dense_109_loss: 20657.2090 - val_loss: 3000471.7500 - val_dense_106_loss: 2976305.2500 - val_dense_109_loss: 24166.4805\n",
      "Epoch 95/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3009291.2500 - dense_106_loss: 2986068.0000 - dense_109_loss: 23223.3379 - val_loss: 2912961.2500 - val_dense_106_loss: 2893718.2500 - val_dense_109_loss: 19243.0938\n",
      "Epoch 96/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3073748.7500 - dense_106_loss: 3053149.2500 - dense_109_loss: 20599.4629 - val_loss: 3010486.2500 - val_dense_106_loss: 2988717.5000 - val_dense_109_loss: 21768.9414\n",
      "Epoch 97/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3093644.7500 - dense_106_loss: 3074225.0000 - dense_109_loss: 19419.4238 - val_loss: 3078360.5000 - val_dense_106_loss: 3050959.2500 - val_dense_109_loss: 27401.4414\n",
      "Epoch 98/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3040654.7500 - dense_106_loss: 3019684.7500 - dense_109_loss: 20969.9043 - val_loss: 3002478.2500 - val_dense_106_loss: 2982847.2500 - val_dense_109_loss: 19631.0352\n",
      "Epoch 99/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3027083.5000 - dense_106_loss: 3002541.2500 - dense_109_loss: 24542.2383 - val_loss: 2976096.0000 - val_dense_106_loss: 2955466.7500 - val_dense_109_loss: 20629.3965\n",
      "Epoch 100/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3117385.0000 - dense_106_loss: 3096469.7500 - dense_109_loss: 20915.6191 - val_loss: 2958989.7500 - val_dense_106_loss: 2937585.5000 - val_dense_109_loss: 21404.4316\n",
      "Epoch 101/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3161351.0000 - dense_106_loss: 3139895.2500 - dense_109_loss: 21455.5859 - val_loss: 3118971.0000 - val_dense_106_loss: 3093973.5000 - val_dense_109_loss: 24997.4199\n",
      "Epoch 102/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3124777.0000 - dense_106_loss: 3104532.5000 - dense_109_loss: 20244.6641 - val_loss: 3076380.0000 - val_dense_106_loss: 3052836.0000 - val_dense_109_loss: 23544.0312\n",
      "Epoch 103/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2993661.5000 - dense_106_loss: 2973598.7500 - dense_109_loss: 20062.8848 - val_loss: 2974012.5000 - val_dense_106_loss: 2953425.7500 - val_dense_109_loss: 20586.6250\n",
      "Epoch 104/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3093846.7500 - dense_106_loss: 3073617.7500 - dense_109_loss: 20229.1484 - val_loss: 2991824.2500 - val_dense_106_loss: 2970938.7500 - val_dense_109_loss: 20885.6426\n",
      "Epoch 105/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3112485.5000 - dense_106_loss: 3092525.0000 - dense_109_loss: 19960.6992 - val_loss: 3081966.0000 - val_dense_106_loss: 3060567.2500 - val_dense_109_loss: 21398.8789\n",
      "Epoch 106/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3077126.7500 - dense_106_loss: 3058309.0000 - dense_109_loss: 18817.6152 - val_loss: 3012089.0000 - val_dense_106_loss: 2990058.0000 - val_dense_109_loss: 22031.2090\n",
      "Epoch 107/1000\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 3029284.2500 - dense_106_loss: 3007645.0000 - dense_109_loss: 21639.0996 - val_loss: 3036327.0000 - val_dense_106_loss: 3017516.2500 - val_dense_109_loss: 18810.5449\n",
      "Epoch 108/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3034623.0000 - dense_106_loss: 3012364.5000 - dense_109_loss: 22258.5762 - val_loss: 3023974.5000 - val_dense_106_loss: 3000987.0000 - val_dense_109_loss: 22987.5332\n",
      "Epoch 109/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3010968.0000 - dense_106_loss: 2990068.7500 - dense_109_loss: 20899.5117 - val_loss: 2981215.2500 - val_dense_106_loss: 2959806.7500 - val_dense_109_loss: 21408.3203\n",
      "Epoch 110/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3019611.0000 - dense_106_loss: 2998088.5000 - dense_109_loss: 21522.5410 - val_loss: 2980750.2500 - val_dense_106_loss: 2955844.7500 - val_dense_109_loss: 24905.5352\n",
      "Epoch 111/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3017837.7500 - dense_106_loss: 2996339.5000 - dense_109_loss: 21498.2305 - val_loss: 2992005.2500 - val_dense_106_loss: 2970799.5000 - val_dense_109_loss: 21205.9570\n",
      "Epoch 112/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 2619229.5000 - dense_106_loss: 2588742.2500 - dense_109_loss: 30487.259 - 0s 3ms/step - loss: 3023991.0000 - dense_106_loss: 3003465.2500 - dense_109_loss: 20525.8574 - val_loss: 2970161.2500 - val_dense_106_loss: 2949911.2500 - val_dense_109_loss: 20249.8418\n",
      "Epoch 113/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3009378.0000 - dense_106_loss: 2988545.7500 - dense_109_loss: 20832.3828 - val_loss: 3005202.0000 - val_dense_106_loss: 2984077.5000 - val_dense_109_loss: 21124.5352\n",
      "Epoch 114/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3033881.0000 - dense_106_loss: 3010750.7500 - dense_109_loss: 23130.3887 - val_loss: 2974483.0000 - val_dense_106_loss: 2955400.5000 - val_dense_109_loss: 19082.8359\n",
      "Epoch 115/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3014701.7500 - dense_106_loss: 2993631.5000 - dense_109_loss: 21070.3457 - val_loss: 2956968.7500 - val_dense_106_loss: 2931156.5000 - val_dense_109_loss: 25812.5957\n",
      "Epoch 116/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 2504488.2500 - dense_106_loss: 2470779.7500 - dense_109_loss: 33708.503 - 0s 3ms/step - loss: 3094779.5000 - dense_106_loss: 3069939.5000 - dense_109_loss: 24839.6934 - val_loss: 2923102.2500 - val_dense_106_loss: 2899250.0000 - val_dense_109_loss: 23852.1875\n",
      "Epoch 117/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2988026.5000 - dense_106_loss: 2961981.7500 - dense_109_loss: 26044.5391 - val_loss: 2972310.7500 - val_dense_106_loss: 2953382.7500 - val_dense_109_loss: 18928.0312\n",
      "Epoch 118/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3034046.7500 - dense_106_loss: 3012687.5000 - dense_109_loss: 21359.4492 - val_loss: 2977148.5000 - val_dense_106_loss: 2953617.0000 - val_dense_109_loss: 23531.4609\n",
      "Epoch 119/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3030627.5000 - dense_106_loss: 3010507.5000 - dense_109_loss: 20119.9727 - val_loss: 2938722.5000 - val_dense_106_loss: 2915071.7500 - val_dense_109_loss: 23650.5859\n",
      "Epoch 120/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3026629.0000 - dense_106_loss: 3006009.0000 - dense_109_loss: 20619.8398 - val_loss: 2899707.5000 - val_dense_106_loss: 2880166.0000 - val_dense_109_loss: 19541.7070\n",
      "Epoch 121/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3043893.0000 - dense_106_loss: 3024228.0000 - dense_109_loss: 19665.1387 - val_loss: 2970586.0000 - val_dense_106_loss: 2947616.0000 - val_dense_109_loss: 22970.0469\n",
      "Epoch 122/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016412.5000 - dense_106_loss: 2996461.0000 - dense_109_loss: 19951.1230 - val_loss: 2891736.0000 - val_dense_106_loss: 2871887.7500 - val_dense_109_loss: 19848.3555\n",
      "Epoch 123/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3022028.0000 - dense_106_loss: 2999595.0000 - dense_109_loss: 22433.1953 - val_loss: 2868561.5000 - val_dense_106_loss: 2848806.2500 - val_dense_109_loss: 19755.0566\n",
      "Epoch 124/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3038314.0000 - dense_106_loss: 3018211.5000 - dense_109_loss: 20102.3457 - val_loss: 2964506.7500 - val_dense_106_loss: 2939857.5000 - val_dense_109_loss: 24649.0723\n",
      "Epoch 125/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3031479.5000 - dense_106_loss: 3010349.0000 - dense_109_loss: 21130.0566 - val_loss: 2929824.0000 - val_dense_106_loss: 2905758.0000 - val_dense_109_loss: 24066.1387\n",
      "Epoch 126/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2984803.0000 - dense_106_loss: 2965612.0000 - dense_109_loss: 19191.1699 - val_loss: 2901845.0000 - val_dense_106_loss: 2881049.7500 - val_dense_109_loss: 20795.0352\n",
      "Epoch 127/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3017860.2500 - dense_106_loss: 2998367.0000 - dense_109_loss: 19493.7168 - val_loss: 2948313.2500 - val_dense_106_loss: 2924075.2500 - val_dense_109_loss: 24237.8457\n",
      "Epoch 128/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3084870.0000 - dense_106_loss: 3063531.0000 - dense_109_loss: 21339.0547 - val_loss: 2974588.0000 - val_dense_106_loss: 2952675.0000 - val_dense_109_loss: 21912.8750\n",
      "Epoch 129/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3023945.0000 - dense_106_loss: 3004041.5000 - dense_109_loss: 19903.4648 - val_loss: 2921550.7500 - val_dense_106_loss: 2897805.5000 - val_dense_109_loss: 23745.0879\n",
      "Epoch 130/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3012114.0000 - dense_106_loss: 2990578.5000 - dense_109_loss: 21535.4688 - val_loss: 2910270.2500 - val_dense_106_loss: 2890501.5000 - val_dense_109_loss: 19768.5781\n",
      "Epoch 131/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3022169.2500 - dense_106_loss: 3001549.0000 - dense_109_loss: 20620.4121 - val_loss: 2931518.5000 - val_dense_106_loss: 2906031.7500 - val_dense_109_loss: 25486.5508\n",
      "Epoch 132/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3026388.0000 - dense_106_loss: 3006831.0000 - dense_109_loss: 19556.8613 - val_loss: 2894075.2500 - val_dense_106_loss: 2873437.0000 - val_dense_109_loss: 20638.1875\n",
      "Epoch 133/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3015214.5000 - dense_106_loss: 2992669.7500 - dense_109_loss: 22544.4824 - val_loss: 2869867.2500 - val_dense_106_loss: 2847899.2500 - val_dense_109_loss: 21967.9746\n",
      "Epoch 134/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3036571.5000 - dense_106_loss: 3015513.7500 - dense_109_loss: 21058.3516 - val_loss: 2950891.0000 - val_dense_106_loss: 2926666.2500 - val_dense_109_loss: 24224.6758\n",
      "Epoch 135/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3033273.0000 - dense_106_loss: 3013226.0000 - dense_109_loss: 20047.3164 - val_loss: 2879040.5000 - val_dense_106_loss: 2859123.7500 - val_dense_109_loss: 19916.8242\n",
      "Epoch 136/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3013428.5000 - dense_106_loss: 2992854.7500 - dense_109_loss: 20573.9141 - val_loss: 2896708.7500 - val_dense_106_loss: 2871616.7500 - val_dense_109_loss: 25091.9180\n",
      "Epoch 137/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3004852.7500 - dense_106_loss: 2985278.5000 - dense_109_loss: 19574.3652 - val_loss: 2901974.2500 - val_dense_106_loss: 2880811.2500 - val_dense_109_loss: 21163.0977\n",
      "Epoch 138/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3038524.7500 - dense_106_loss: 3015592.5000 - dense_109_loss: 22932.4512 - val_loss: 2913327.0000 - val_dense_106_loss: 2892396.7500 - val_dense_109_loss: 20930.4258\n",
      "Epoch 139/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016174.7500 - dense_106_loss: 2996062.0000 - dense_109_loss: 20112.6016 - val_loss: 2856272.7500 - val_dense_106_loss: 2828807.7500 - val_dense_109_loss: 27465.0410\n",
      "Epoch 140/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2996427.7500 - dense_106_loss: 2976923.2500 - dense_109_loss: 19504.1387 - val_loss: 2939873.0000 - val_dense_106_loss: 2921602.7500 - val_dense_109_loss: 18270.0234\n",
      "Epoch 141/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3186829.0000 - dense_106_loss: 3165758.7500 - dense_109_loss: 21070.0938 - val_loss: 2982927.0000 - val_dense_106_loss: 2961511.7500 - val_dense_109_loss: 21415.4434\n",
      "Epoch 142/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3061509.7500 - dense_106_loss: 3042346.5000 - dense_109_loss: 19163.6875 - val_loss: 2879612.0000 - val_dense_106_loss: 2859097.2500 - val_dense_109_loss: 20514.4824\n",
      "Epoch 143/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3012795.0000 - dense_106_loss: 2993243.7500 - dense_109_loss: 19550.8145 - val_loss: 2910101.0000 - val_dense_106_loss: 2881200.2500 - val_dense_109_loss: 28900.5449\n",
      "Epoch 144/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3036713.0000 - dense_106_loss: 3015593.7500 - dense_109_loss: 21119.6328 - val_loss: 2899941.5000 - val_dense_106_loss: 2877592.2500 - val_dense_109_loss: 22349.0234\n",
      "Epoch 145/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3062885.0000 - dense_106_loss: 3040028.7500 - dense_109_loss: 22856.5918 - val_loss: 2887935.2500 - val_dense_106_loss: 2868332.0000 - val_dense_109_loss: 19603.2969\n",
      "Epoch 146/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3027822.5000 - dense_106_loss: 3001920.0000 - dense_109_loss: 25902.5664 - val_loss: 2939435.7500 - val_dense_106_loss: 2913884.5000 - val_dense_109_loss: 25551.3184\n",
      "Epoch 147/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016307.5000 - dense_106_loss: 2997660.5000 - dense_109_loss: 18647.2422 - val_loss: 2888488.5000 - val_dense_106_loss: 2869485.5000 - val_dense_109_loss: 19003.2910\n",
      "Epoch 148/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2999639.0000 - dense_106_loss: 2979167.2500 - dense_109_loss: 20471.4102 - val_loss: 2878798.2500 - val_dense_106_loss: 2856881.7500 - val_dense_109_loss: 21916.4805\n",
      "Epoch 149/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3000731.2500 - dense_106_loss: 2980499.0000 - dense_109_loss: 20232.2852 - val_loss: 2946959.5000 - val_dense_106_loss: 2923240.5000 - val_dense_109_loss: 23719.1055\n",
      "Epoch 150/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3047398.2500 - dense_106_loss: 3027657.5000 - dense_109_loss: 19740.7344 - val_loss: 2929936.2500 - val_dense_106_loss: 2908312.7500 - val_dense_109_loss: 21623.2578\n",
      "Epoch 151/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2991706.7500 - dense_106_loss: 2971463.5000 - dense_109_loss: 20243.1016 - val_loss: 2881483.0000 - val_dense_106_loss: 2856720.2500 - val_dense_109_loss: 24762.5293\n",
      "Epoch 152/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016696.5000 - dense_106_loss: 2996043.7500 - dense_109_loss: 20652.3633 - val_loss: 2908810.5000 - val_dense_106_loss: 2887015.0000 - val_dense_109_loss: 21795.4570\n",
      "Epoch 153/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016246.0000 - dense_106_loss: 2996202.7500 - dense_109_loss: 20043.3672 - val_loss: 2901112.0000 - val_dense_106_loss: 2880628.5000 - val_dense_109_loss: 20483.5977\n",
      "Epoch 154/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2996184.7500 - dense_106_loss: 2977507.7500 - dense_109_loss: 18676.9043 - val_loss: 2884738.2500 - val_dense_106_loss: 2859776.7500 - val_dense_109_loss: 24961.4492\n",
      "Epoch 155/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3010922.7500 - dense_106_loss: 2990527.7500 - dense_109_loss: 20394.8984 - val_loss: 2892161.5000 - val_dense_106_loss: 2870404.7500 - val_dense_109_loss: 21756.7012\n",
      "Epoch 156/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3058040.5000 - dense_106_loss: 3038450.7500 - dense_109_loss: 19589.5039 - val_loss: 2918447.2500 - val_dense_106_loss: 2895089.5000 - val_dense_109_loss: 23357.8242\n",
      "Epoch 157/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3032251.5000 - dense_106_loss: 3012269.2500 - dense_109_loss: 19981.8711 - val_loss: 2874914.5000 - val_dense_106_loss: 2853950.7500 - val_dense_109_loss: 20963.5039\n",
      "Epoch 158/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3041150.0000 - dense_106_loss: 3020426.5000 - dense_109_loss: 20723.4883 - val_loss: 2919837.2500 - val_dense_106_loss: 2897653.7500 - val_dense_109_loss: 22183.5508\n",
      "Epoch 159/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3027982.2500 - dense_106_loss: 3004237.7500 - dense_109_loss: 23744.2578 - val_loss: 2879388.7500 - val_dense_106_loss: 2859223.5000 - val_dense_109_loss: 20165.1270\n",
      "Epoch 160/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2989074.2500 - dense_106_loss: 2967890.7500 - dense_109_loss: 21183.6836 - val_loss: 2929742.0000 - val_dense_106_loss: 2903343.5000 - val_dense_109_loss: 26398.6230\n",
      "Epoch 161/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3023022.7500 - dense_106_loss: 3004000.0000 - dense_109_loss: 19022.5078 - val_loss: 2878498.5000 - val_dense_106_loss: 2859046.7500 - val_dense_109_loss: 19451.6855\n",
      "Epoch 162/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3001763.5000 - dense_106_loss: 2981288.2500 - dense_109_loss: 20474.8574 - val_loss: 2896780.5000 - val_dense_106_loss: 2870224.0000 - val_dense_109_loss: 26556.2520\n",
      "Epoch 163/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3015029.5000 - dense_106_loss: 2994260.5000 - dense_109_loss: 20769.4648 - val_loss: 2841494.0000 - val_dense_106_loss: 2822292.5000 - val_dense_109_loss: 19201.6035\n",
      "Epoch 164/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3019799.7500 - dense_106_loss: 2999251.0000 - dense_109_loss: 20548.5859 - val_loss: 2914872.0000 - val_dense_106_loss: 2891170.2500 - val_dense_109_loss: 23701.6562\n",
      "Epoch 165/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3027015.0000 - dense_106_loss: 3008116.7500 - dense_109_loss: 18898.3848 - val_loss: 2881418.5000 - val_dense_106_loss: 2861346.7500 - val_dense_109_loss: 20072.0352\n",
      "Epoch 166/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3005415.2500 - dense_106_loss: 2985269.0000 - dense_109_loss: 20146.2988 - val_loss: 2841700.5000 - val_dense_106_loss: 2821901.5000 - val_dense_109_loss: 19798.8477\n",
      "Epoch 167/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3038070.0000 - dense_106_loss: 3017903.2500 - dense_109_loss: 20166.6250 - val_loss: 2913222.7500 - val_dense_106_loss: 2892757.2500 - val_dense_109_loss: 20465.5020\n",
      "Epoch 168/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016906.7500 - dense_106_loss: 2997813.7500 - dense_109_loss: 19093.0527 - val_loss: 2892765.7500 - val_dense_106_loss: 2872305.5000 - val_dense_109_loss: 20459.9922\n",
      "Epoch 169/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3035444.2500 - dense_106_loss: 3015508.7500 - dense_109_loss: 19935.4004 - val_loss: 2888356.5000 - val_dense_106_loss: 2868426.2500 - val_dense_109_loss: 19929.8125\n",
      "Epoch 170/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3062383.5000 - dense_106_loss: 3042340.5000 - dense_109_loss: 20043.1973 - val_loss: 2968486.7500 - val_dense_106_loss: 2943901.0000 - val_dense_109_loss: 24585.7344\n",
      "Epoch 171/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3079969.0000 - dense_106_loss: 3060148.7500 - dense_109_loss: 19820.3535 - val_loss: 2907046.0000 - val_dense_106_loss: 2887236.5000 - val_dense_109_loss: 19809.4414\n",
      "Epoch 172/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 7788654.5000 - dense_106_loss: 7728806.5000 - dense_109_loss: 59848.046 - 0s 3ms/step - loss: 3107725.0000 - dense_106_loss: 3086424.7500 - dense_109_loss: 21300.1621 - val_loss: 2841349.0000 - val_dense_106_loss: 2818100.0000 - val_dense_109_loss: 23248.8418\n",
      "Epoch 173/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3055563.5000 - dense_106_loss: 3035375.2500 - dense_109_loss: 20188.3652 - val_loss: 2954311.7500 - val_dense_106_loss: 2931154.2500 - val_dense_109_loss: 23157.4941\n",
      "Epoch 174/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3056093.7500 - dense_106_loss: 3035433.2500 - dense_109_loss: 20660.4688 - val_loss: 2868204.0000 - val_dense_106_loss: 2848367.0000 - val_dense_109_loss: 19837.1641\n",
      "Epoch 175/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3033856.0000 - dense_106_loss: 3010589.7500 - dense_109_loss: 23266.0879 - val_loss: 2927665.7500 - val_dense_106_loss: 2903318.5000 - val_dense_109_loss: 24347.4336\n",
      "Epoch 176/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3016829.0000 - dense_106_loss: 2996409.5000 - dense_109_loss: 20419.3613 - val_loss: 2913468.2500 - val_dense_106_loss: 2893972.7500 - val_dense_109_loss: 19495.6055\n",
      "Epoch 177/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2998361.2500 - dense_106_loss: 2978769.0000 - dense_109_loss: 19592.8008 - val_loss: 2864365.0000 - val_dense_106_loss: 2836392.2500 - val_dense_109_loss: 27972.6152\n",
      "Epoch 178/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3001954.5000 - dense_106_loss: 2978509.2500 - dense_109_loss: 23445.0645 - val_loss: 2871105.0000 - val_dense_106_loss: 2851725.2500 - val_dense_109_loss: 19379.5273\n",
      "Epoch 179/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3000151.0000 - dense_106_loss: 2978111.0000 - dense_109_loss: 22040.4160 - val_loss: 2905012.0000 - val_dense_106_loss: 2879396.5000 - val_dense_109_loss: 25615.2012\n",
      "Epoch 180/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2997827.2500 - dense_106_loss: 2978174.7500 - dense_109_loss: 19652.6191 - val_loss: 2878503.5000 - val_dense_106_loss: 2860394.7500 - val_dense_109_loss: 18108.4844\n",
      "Epoch 181/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3031090.5000 - dense_106_loss: 3006695.0000 - dense_109_loss: 24395.2734 - val_loss: 2912148.2500 - val_dense_106_loss: 2888541.2500 - val_dense_109_loss: 23606.8008\n",
      "Epoch 182/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3068391.0000 - dense_106_loss: 3047182.0000 - dense_109_loss: 21208.9316 - val_loss: 2952358.7500 - val_dense_106_loss: 2930820.0000 - val_dense_109_loss: 21538.7969\n",
      "Epoch 183/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 2996979.0000 - dense_106_loss: 2976308.0000 - dense_109_loss: 20671.1934 - val_loss: 2853384.2500 - val_dense_106_loss: 2827044.5000 - val_dense_109_loss: 26339.8691\n",
      "Epoch 184/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 3108843.2500 - dense_106_loss: 3098358.0000 - dense_109_loss: 10485.234 - 0s 3ms/step - loss: 3047692.2500 - dense_106_loss: 3028597.7500 - dense_109_loss: 19094.3359 - val_loss: 2929755.7500 - val_dense_106_loss: 2910616.7500 - val_dense_109_loss: 19138.9609\n",
      "Epoch 185/1000\n",
      "9/9 [==============================] - ETA: 0s - loss: 3139222.2500 - dense_106_loss: 3121481.0000 - dense_109_loss: 17741.341 - 0s 3ms/step - loss: 2999211.0000 - dense_106_loss: 2980927.2500 - dense_109_loss: 18283.3984 - val_loss: 2943347.7500 - val_dense_106_loss: 2919880.0000 - val_dense_109_loss: 23467.7559\n",
      "Epoch 186/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3019311.0000 - dense_106_loss: 2997205.2500 - dense_109_loss: 22105.6094 - val_loss: 2914201.7500 - val_dense_106_loss: 2894940.2500 - val_dense_109_loss: 19261.4980\n",
      "Epoch 187/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3013135.0000 - dense_106_loss: 2993173.0000 - dense_109_loss: 19962.3672 - val_loss: 2906230.5000 - val_dense_106_loss: 2883491.0000 - val_dense_109_loss: 22739.3828\n",
      "Epoch 188/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3037185.0000 - dense_106_loss: 3017922.7500 - dense_109_loss: 19262.3789 - val_loss: 2831489.5000 - val_dense_106_loss: 2812573.2500 - val_dense_109_loss: 18916.3418\n",
      "Epoch 189/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3687459.0000 - dense_106_loss: 3665882.2500 - dense_109_loss: 21576.7930 - val_loss: 3007089.2500 - val_dense_106_loss: 2987101.0000 - val_dense_109_loss: 19988.2969\n",
      "Epoch 190/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3739293.7500 - dense_106_loss: 3716841.5000 - dense_109_loss: 22452.1699 - val_loss: 3009781.0000 - val_dense_106_loss: 2990583.2500 - val_dense_109_loss: 19197.6074\n",
      "Epoch 191/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3669895.2500 - dense_106_loss: 3647138.7500 - dense_109_loss: 22756.6875 - val_loss: 2689085.5000 - val_dense_106_loss: 2662065.5000 - val_dense_109_loss: 27019.8672\n",
      "Epoch 192/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3565766.2500 - dense_106_loss: 3540548.5000 - dense_109_loss: 25217.4609 - val_loss: 2714792.5000 - val_dense_106_loss: 2694797.2500 - val_dense_109_loss: 19995.4707\n",
      "Epoch 193/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3516919.0000 - dense_106_loss: 3495792.5000 - dense_109_loss: 21126.5957 - val_loss: 2900756.5000 - val_dense_106_loss: 2881589.2500 - val_dense_109_loss: 19167.1973\n",
      "Epoch 194/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3564727.2500 - dense_106_loss: 3541618.5000 - dense_109_loss: 23108.5508 - val_loss: 3102905.5000 - val_dense_106_loss: 3080734.0000 - val_dense_109_loss: 22171.1621\n",
      "Epoch 195/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3405364.7500 - dense_106_loss: 3385705.2500 - dense_109_loss: 19659.1289 - val_loss: 3301427.5000 - val_dense_106_loss: 3282298.7500 - val_dense_109_loss: 19128.8770\n",
      "Epoch 196/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3323056.2500 - dense_106_loss: 3302749.5000 - dense_109_loss: 20306.4785 - val_loss: 3467557.7500 - val_dense_106_loss: 3445134.0000 - val_dense_109_loss: 22423.6133\n",
      "Epoch 197/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3321993.7500 - dense_106_loss: 3302404.2500 - dense_109_loss: 19589.4551 - val_loss: 3524813.5000 - val_dense_106_loss: 3506005.7500 - val_dense_109_loss: 18807.3750\n",
      "Epoch 198/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3338609.2500 - dense_106_loss: 3316852.0000 - dense_109_loss: 21757.2031 - val_loss: 3573482.5000 - val_dense_106_loss: 3548385.2500 - val_dense_109_loss: 25096.9453\n",
      "Epoch 199/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3337026.5000 - dense_106_loss: 3317034.2500 - dense_109_loss: 19992.2578 - val_loss: 3461140.7500 - val_dense_106_loss: 3443260.2500 - val_dense_109_loss: 17880.4570\n",
      "Epoch 200/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3340073.2500 - dense_106_loss: 3314653.0000 - dense_109_loss: 25420.3926 - val_loss: 3327858.5000 - val_dense_106_loss: 3308662.7500 - val_dense_109_loss: 19195.9316\n",
      "Epoch 201/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3331581.0000 - dense_106_loss: 3312990.2500 - dense_109_loss: 18590.7402 - val_loss: 3327689.0000 - val_dense_106_loss: 3299636.0000 - val_dense_109_loss: 28053.1016\n",
      "Epoch 202/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3352550.2500 - dense_106_loss: 3329978.7500 - dense_109_loss: 22571.4121 - val_loss: 3534193.0000 - val_dense_106_loss: 3517184.7500 - val_dense_109_loss: 17008.2578\n",
      "Epoch 203/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3335081.2500 - dense_106_loss: 3308231.7500 - dense_109_loss: 26849.6133 - val_loss: 3490600.7500 - val_dense_106_loss: 3473149.5000 - val_dense_109_loss: 17451.2617\n",
      "Epoch 204/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3356463.0000 - dense_106_loss: 3336210.5000 - dense_109_loss: 20252.9980 - val_loss: 3326926.5000 - val_dense_106_loss: 3301957.7500 - val_dense_109_loss: 24968.4824\n",
      "Epoch 205/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3321080.5000 - dense_106_loss: 3300463.2500 - dense_109_loss: 20616.9141 - val_loss: 3381306.2500 - val_dense_106_loss: 3362115.7500 - val_dense_109_loss: 19190.2422\n",
      "Epoch 206/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3314397.0000 - dense_106_loss: 3293547.5000 - dense_109_loss: 20849.4922 - val_loss: 3410661.5000 - val_dense_106_loss: 3385496.5000 - val_dense_109_loss: 25164.9512\n",
      "Epoch 207/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3358006.0000 - dense_106_loss: 3336256.5000 - dense_109_loss: 21749.6758 - val_loss: 3500158.0000 - val_dense_106_loss: 3479801.7500 - val_dense_109_loss: 20356.5371\n",
      "Epoch 208/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3330651.2500 - dense_106_loss: 3308543.7500 - dense_109_loss: 22106.9766 - val_loss: 3427144.5000 - val_dense_106_loss: 3410036.0000 - val_dense_109_loss: 17108.2480\n",
      "Epoch 209/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3321488.2500 - dense_106_loss: 3299711.0000 - dense_109_loss: 21777.0859 - val_loss: 3355557.5000 - val_dense_106_loss: 3334024.7500 - val_dense_109_loss: 21532.8184\n",
      "Epoch 210/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3324064.2500 - dense_106_loss: 3302637.0000 - dense_109_loss: 21426.8828 - val_loss: 3341425.5000 - val_dense_106_loss: 3320803.2500 - val_dense_109_loss: 20622.0293\n",
      "Epoch 211/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3323662.5000 - dense_106_loss: 3302952.5000 - dense_109_loss: 20709.9570 - val_loss: 3354023.0000 - val_dense_106_loss: 3331494.0000 - val_dense_109_loss: 22528.9336\n",
      "Epoch 212/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3319107.0000 - dense_106_loss: 3297184.0000 - dense_109_loss: 21922.9141 - val_loss: 3425532.5000 - val_dense_106_loss: 3400079.5000 - val_dense_109_loss: 25453.3125\n",
      "Epoch 213/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3347212.0000 - dense_106_loss: 3325444.2500 - dense_109_loss: 21767.7734 - val_loss: 3359734.7500 - val_dense_106_loss: 3342749.2500 - val_dense_109_loss: 16985.6465\n",
      "Epoch 214/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3333701.0000 - dense_106_loss: 3312664.7500 - dense_109_loss: 21036.4746 - val_loss: 3334786.0000 - val_dense_106_loss: 3312223.2500 - val_dense_109_loss: 22562.5176\n",
      "Epoch 215/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3309018.0000 - dense_106_loss: 3289088.0000 - dense_109_loss: 19929.9512 - val_loss: 3416458.2500 - val_dense_106_loss: 3398167.7500 - val_dense_109_loss: 18290.4258\n",
      "Epoch 216/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3344633.7500 - dense_106_loss: 3324460.0000 - dense_109_loss: 20174.4727 - val_loss: 3485847.5000 - val_dense_106_loss: 3466746.2500 - val_dense_109_loss: 19101.3164\n",
      "Epoch 217/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3333551.0000 - dense_106_loss: 3311857.5000 - dense_109_loss: 21693.3828 - val_loss: 3453507.5000 - val_dense_106_loss: 3430124.7500 - val_dense_109_loss: 23382.4258\n",
      "Epoch 218/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3309949.2500 - dense_106_loss: 3287913.5000 - dense_109_loss: 22035.5156 - val_loss: 3358855.5000 - val_dense_106_loss: 3340111.0000 - val_dense_109_loss: 18744.6309\n",
      "Epoch 219/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3313497.2500 - dense_106_loss: 3291748.5000 - dense_109_loss: 21748.7090 - val_loss: 3311455.5000 - val_dense_106_loss: 3294155.2500 - val_dense_109_loss: 17300.2793\n",
      "Epoch 220/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3327676.5000 - dense_106_loss: 3305356.0000 - dense_109_loss: 22320.2461 - val_loss: 3395840.2500 - val_dense_106_loss: 3371127.7500 - val_dense_109_loss: 24712.7383\n",
      "Epoch 221/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3313628.2500 - dense_106_loss: 3294266.0000 - dense_109_loss: 19361.9609 - val_loss: 3345401.7500 - val_dense_106_loss: 3326857.5000 - val_dense_109_loss: 18544.1406\n",
      "Epoch 222/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3333039.0000 - dense_106_loss: 3307123.2500 - dense_109_loss: 25916.0918 - val_loss: 3310404.2500 - val_dense_106_loss: 3290024.2500 - val_dense_109_loss: 20380.1191\n",
      "Epoch 223/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3329333.0000 - dense_106_loss: 3306945.7500 - dense_109_loss: 22386.7012 - val_loss: 3425751.0000 - val_dense_106_loss: 3401165.7500 - val_dense_109_loss: 24585.0312\n",
      "Epoch 224/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3317109.0000 - dense_106_loss: 3296543.5000 - dense_109_loss: 20565.0332 - val_loss: 3397254.0000 - val_dense_106_loss: 3377400.7500 - val_dense_109_loss: 19853.1094\n",
      "Epoch 225/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3329928.5000 - dense_106_loss: 3310583.0000 - dense_109_loss: 19345.1719 - val_loss: 3336614.0000 - val_dense_106_loss: 3316640.0000 - val_dense_109_loss: 19973.6719\n",
      "Epoch 226/1000\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 3327138.2500 - dense_106_loss: 3306841.2500 - dense_109_loss: 20297.4766 - val_loss: 3397133.5000 - val_dense_106_loss: 3379131.0000 - val_dense_109_loss: 18002.5918\n",
      "Epoch 227/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3317213.5000 - dense_106_loss: 3296034.0000 - dense_109_loss: 21179.7891 - val_loss: 3330767.5000 - val_dense_106_loss: 3312370.5000 - val_dense_109_loss: 18397.0078\n",
      "Epoch 228/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3296301.5000 - dense_106_loss: 3274903.0000 - dense_109_loss: 21398.5898 - val_loss: 3430203.5000 - val_dense_106_loss: 3408529.5000 - val_dense_109_loss: 21674.1543\n",
      "Epoch 229/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3386629.0000 - dense_106_loss: 3366932.2500 - dense_109_loss: 19696.4980 - val_loss: 3529914.0000 - val_dense_106_loss: 3512981.0000 - val_dense_109_loss: 16933.3750\n",
      "Epoch 230/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3379978.7500 - dense_106_loss: 3358066.0000 - dense_109_loss: 21912.8828 - val_loss: 3344346.0000 - val_dense_106_loss: 3325831.7500 - val_dense_109_loss: 18514.1719\n",
      "Epoch 231/1000\n",
      "9/9 [==============================] - 0s 3ms/step - loss: 3322828.0000 - dense_106_loss: 3298441.2500 - dense_109_loss: 24386.6113 - val_loss: 3379491.5000 - val_dense_106_loss: 3354877.2500 - val_dense_109_loss: 24614.1211\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', patience=40, restore_best_weights=True)\n",
    "hist = model.fit([x1_train,x2_train], [y1_train, y2_train], epochs=1000, batch_size=1,\n",
    "                 validation_split=0.3, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "평가 & 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step - loss: 4517233.5000 - dense_106_loss: 4484132.0000 - dense_109_loss: 33101.5820\n",
      "loss :  [4517233.5, 4484132.0, 33101.58203125]\n",
      "예측값 :  [10809440. 10411019.  8638873.] [37637.652 21412.5   21083.12 ]\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate([x1_test, x2_test], [y1_test, y2_test])\n",
    "print('loss : ', loss)\n",
    "ss_pred, ki_pred = model.predict([x1_test, x2_test])\n",
    "print('예측값 : ', ss_pred[-1], ki_pred[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"../_save/stock_tue_vol_{}.h5\".format(ss_pred[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dense_6_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16532/1519759607.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mss_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dense_6_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mku_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dense_9_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mss_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_dense_6_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mku_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val_dense_9_loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dense_6_loss'"
     ]
    }
   ],
   "source": [
    "loss = hist.history[\"loss\"]\n",
    "ss_loss = hist.history[\"dense_6_loss\"]\n",
    "ku_loss = hist.history[\"dense_9_loss\"]\n",
    "ss_val = hist.history[\"val_dense_6_loss\"]\n",
    "ku_val = hist.history[\"val_dense_9_loss\"]\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "\n",
    "plt.plot(epochs, loss, 'y--', label=\"training loss\")\n",
    "plt.plot(epochs, ss_loss, 'r--', label=\"ss_loss\")\n",
    "plt.plot(epochs, ku_loss, 'b--', label=\"ku_loss\")\n",
    "plt.plot(epochs, ss_val, 'r:', label=\"ss_val\")\n",
    "plt.plot(epochs, ku_val, 'b:', label=\"ku_val\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 234ms/step - loss: 5573683.5000 - dense_316_loss: 5526711.0000 - dense_319_loss: 46972.2891\n",
      "loss :  [5573683.5, 5526711.0, 46972.2890625]\n",
      "예측값 :  10002156.0 62839.086\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"../_save\\stock_tue_vol_[10002156.].h5\")\n",
    "\n",
    "loss = model.evaluate([x1_test, x2_test], [y1_test, y2_test])\n",
    "print('loss : ', loss)\n",
    "ss_pred, ki_pred = model.predict([x1_test, x2_test])\n",
    "print('예측값 : ', ss_pred[-1][-1], ki_pred[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
