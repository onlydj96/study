{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스로 Word2Vec 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, \n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(d):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', d)\n",
    "    return text\n",
    "\n",
    "def clean_stopword(d):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w.lower() for w in d.split() if w not in stop_words and len(w) > 3])\n",
    "\n",
    "def tokenize(d):\n",
    "    return word_tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11096"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 판다스 데이터프레임으로 변환\n",
    "import pandas as pd\n",
    "news_df = pd.DataFrame({'article' : documents})\n",
    "\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.dropna(inplace=True)\n",
    "len(news_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식 적용\n",
    "news_df['article'] = news_df['article'].apply(clean_text)\n",
    "\n",
    "# 불용어 제거\n",
    "news_df['article'] = news_df['article'].apply(clean_stopword)\n",
    "\n",
    "# 토크나이즈\n",
    "tokenized_news = news_df['article'].apply(tokenize)\n",
    "tokenized_news = tokenized_news.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10945"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <= 1]\n",
    "news_texts = np.delete(tokenized_news, drop_news, axis=0)\n",
    "len(news_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "news_2000 = news_texts[:2000]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(news_2000)\n",
    "\n",
    "idx2word = {value:key for key, value in tokenizer.word_index.items()}\n",
    "sequences = tokenizer.texts_to_sequences(news_2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29769\n",
      "[1263, 457, 2, 60, 119, 419, 61, 1374, 22, 69, 3498, 397, 6874, 412, 1173, 373, 2256, 458, 59, 12478, 458, 1900, 3850, 397, 22, 10, 4325, 8749, 177, 303, 136, 154, 664, 12479, 316, 12480, 15, 12481, 4, 790, 12482, 12483, 4917, 8750]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(sequences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 네거티브 샘플링(Negative Sampling)\n",
    "     - Word2Vec은 출력층이 내놓는 값에 소프트맥스 함수를 적용해 학률값으로 변환한 후 이를 정답과 비교해 역전파(Backpropagation)\n",
    "     - 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심단어와 나머지 모든 단어의 내적을 한 뒤 이를 다시 exp 계산을 하는데 전체 단어가 많을 경우 엄청난 계산량 발생\n",
    "     - 네거티브 샘플링은 소프트맥스 확률을 구할 때 전체 단어를 대상으로 구하지 않고, 일부 단어만 뽑아서 계산을 하는 방식\n",
    "     - 네거티브 샘플링 동작은 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(negative sample)를 5~20개 정도 뽑고, 이를 정답 단어와 합쳐 전체 단어처럼 소프트맥스 확률을 계산하여 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "think(7), atrocities(4915) -> 1\n",
      "ruin(12474), devilschicagoblackhawks(27523) -> 0\n",
      "commited(4916), soldiers(864) -> 1\n",
      "existance(2744), least(114) -> 1\n",
      "report(627), what(34) -> 1\n",
      "10\n",
      "2420\n",
      "2420\n"
     ]
    }
   ],
   "source": [
    "# 샘플링\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in sequences[:10]]\n",
    "\n",
    " \n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"{:s}({:d}), {:s}({:d}) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]\n",
    "    ))\n",
    "    \n",
    "print(len(skip_grams))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(seq, vocabulary_size=vocab_size, window_size=10) for seq in sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Skipgram 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Activation, Input, Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "embed_size = 50\n",
    "\n",
    "# 모델 생성\n",
    "def word2vec():\n",
    "    target_inputs = Input(shape=(1, ), dtype='int32')\n",
    "    target_embedding = Embedding(vocab_size, embed_size)(target_inputs)\n",
    "    \n",
    "    context_inputs = Input(shape=(1, ), dtype='int32')\n",
    "    context_embedding = Embedding(vocab_size, embed_size)(context_inputs)\n",
    "    \n",
    "    dot_product = Dot(axes=2)([target_embedding, context_embedding])\n",
    "    dot_product = Reshape((1,), input_shape=(1, 1))(dot_product) \n",
    "    output = Activation('sigmoid')(dot_product)\n",
    "    \n",
    "    model = Model(inputs=[target_inputs, context_inputs], outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 50)        1488450     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 50)        1488450     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1, 1)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 1)            0           ['dot[0][0]']                    \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 1)            0           ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,976,900\n",
      "Trainable params: 2,976,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = word2vec()\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 Loss :  1224.5839930176735\n",
      "Epoch :  2 Loss :  923.0617003887892\n",
      "Epoch :  3 Loss :  826.4403100013733\n",
      "Epoch :  4 Loss :  781.8529791980982\n",
      "Epoch :  5 Loss :  756.450382605195\n",
      "Epoch :  6 Loss :  737.7231607586145\n",
      "Epoch :  7 Loss :  719.2551698684692\n",
      "Epoch :  8 Loss :  697.2652211561799\n",
      "Epoch :  9 Loss :  670.0547076575458\n",
      "Epoch :  10 Loss :  637.1437278930098\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "for epoch in range(1, 11):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X, Y)\n",
    "        \n",
    "    print('Epoch : ', epoch, \"Loss : \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('skipgram.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "skipgrams = gensim.models.KeyedVectors.load_word2vec_format('skipgram.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rocketship', 0.557788074016571),\n",
       " ('apparent', 0.5494934320449829),\n",
       " ('meaxtfcxcxtexcfxcbnxxixte', 0.538479745388031),\n",
       " ('myriads', 0.5340452790260315),\n",
       " ('grider', 0.5026323795318604),\n",
       " ('eaten', 0.5016666054725647),\n",
       " ('laomer', 0.492244154214859),\n",
       " ('missouri', 0.4906013309955597),\n",
       " ('homosexuals', 0.4789530634880066),\n",
       " ('mussinas', 0.47832080721855164)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams.most_similar(positive=['soldier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xtry', 0.5104212760925293),\n",
       " ('edelweiss', 0.4897487461566925),\n",
       " ('athens', 0.48410314321517944),\n",
       " ('appears', 0.4814095199108124),\n",
       " ('wordings', 0.4810832142829895),\n",
       " ('breakfast', 0.47731584310531616),\n",
       " ('jesuss', 0.47518065571784973),\n",
       " ('oftentimes', 0.47448301315307617),\n",
       " ('abhor', 0.47324761748313904),\n",
       " ('jointly', 0.4714374542236328)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams.most_similar(positive=['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - CBOW 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram2cbow(skipgrams):\n",
    "    cbows = []\n",
    "    flag = 0\n",
    "    for n in skipgrams:\n",
    "        temp1 = []\n",
    "        for t in n:\n",
    "            if flag == 1:\n",
    "                flag = 0\n",
    "                temp1.append(t)\n",
    "            else:\n",
    "                flag = 1\n",
    "                temp2 = []\n",
    "                for x in t:\n",
    "                    temp2.append([x[1], x[0]])\n",
    "                temp1.append(temp2)\n",
    "        cbows.append(temp1)\n",
    "    return cbows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbows = skipgram2cbow(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walkertiscom(17317), realize(966) -> 0\n",
      "avengers(7744), what(34) -> 0\n",
      "mlohfvcpj(26153), after(346) -> 0\n",
      "proisraeli(12476), rediculous(12475) -> 1\n",
      "treating(3849), received(387) -> 1\n",
      "2000\n",
      "2420\n",
      "2420\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = cbows[0][0], cbows[0][1]\n",
    "for i in range(5):\n",
    "    print(\"{:s}({:d}), {:s}({:d}) -> {:d}\".format(\n",
    "        idx2word[pairs[i][0]], pairs[i][0],\n",
    "        idx2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i]))\n",
    "\n",
    "print(len(cbows))\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - CBOW 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 50)        1488450     ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 50)        1488450     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " dot_1 (Dot)                    (None, 1, 1)         0           ['embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 1)            0           ['dot_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 1)            0           ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,976,900\n",
      "Trainable params: 2,976,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "model = word2vec()\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 Loss :  1226.4911558181047\n",
      "Epoch :  2 Loss :  924.3692298233509\n",
      "Epoch :  3 Loss :  826.8954248577356\n",
      "Epoch :  4 Loss :  781.359145000577\n",
      "Epoch :  5 Loss :  754.5315544530749\n",
      "Epoch :  6 Loss :  733.9005327746272\n",
      "Epoch :  7 Loss :  713.3908961378038\n",
      "Epoch :  8 Loss :  689.6724891737103\n",
      "Epoch :  9 Loss :  661.4013098366559\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "for epoch in range(1, 10):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(cbows):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X, Y)\n",
    "        \n",
    "    print('Epoch : ', epoch, \"Loss : \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "f = open('cbow.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "skipgrams = gensim.models.KeyedVectors.load_word2vec_format('cbow.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbows.most_similar(positive=['soldier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbows.most_similar(positive=['word'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3caf13703c5b1c02abff9fa597e671e1239d1d668b6a345ae62ddadff9d8fc63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
