{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 분류 등과 같이 별도의 레이블이 없이 텍스트 자체만 있어도 학습이 가능\n",
    "   - Word2Vec의 방식은 주변 관계를 이용하는 것으로 2가지의 방식이 있음\n",
    "   - 1. CBOW(continuous Bag-of-words) : 주변 단어의 임베딩을 더해서 대상단어를 예측\n",
    "   - 2. Skip-Gram : 대상 단어의 임베딩으로 주변단어를 예측\n",
    "      - 일반적으로 CBOW보다 성능이 좋은 편이지만 한번에 여러 단어를 예측해야하기 때문에 비효율적\n",
    "      - 최근에는 negative sampling이라는 방법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-SNE(t-Stochastic Neighbor Embedding)\n",
    "   - T-SNE은 고차원의 벡터들의 구조를 보존하며 저차원으로 사상하는 차원 축소 알고리즘\n",
    "   - 단어 임베딩에서도 생성된 고차원 벡터들을 시각화하기 위해 T-SNE 알고이즘을 많이 이용\n",
    "   - t-sne는 가장 먼저 원 공간의 데이터 유사도와 임베딩 공간의 데이터 유사도를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, \n",
    "                             remove=('headers', 'footers', 'quotes'))   # word2vec하는거라 필요없는 것들을 제거\n",
    "\n",
    "documents = dataset.data\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(d):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', d)\n",
    "    return d\n",
    "\n",
    "# 불용어 제거\n",
    "def clean_stopword(d):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w.lower() for w in d.split() if w not in stop_words and len(w) > 3])\n",
    "\n",
    "def tokenize(d):\n",
    "    return word_tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11096\n"
     ]
    }
   ],
   "source": [
    "# Pandas Frame으로 변환\n",
    "import pandas as pd\n",
    "news_df = pd.DataFrame({'article':documents})\n",
    "\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.dropna(inplace=True)\n",
    "print(len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "news_df['article'] = news_df['article'].apply(clean_text)\n",
    "\n",
    "# 불용어 제거\n",
    "news_df['article'] = news_df['article'].apply(clean_stopword)\n",
    "\n",
    "# 토크나이저\n",
    "tokenized_news = news_df['article'].apply(tokenize)\n",
    "tokenized_news = tokenized_news.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <= 1]\n",
    "news_texts = np.delete(tokenized_news, drop_news, axis=0)\n",
    "print(len(news_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim을 이용한 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85426056"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=news_texts, window=3, vector_size=100, min_count=5, workers=4, sg=0)\n",
    "\n",
    "# window : 앞뒤로 몇개의 단어를 보고 유추할 것인가? 보통 3,4\n",
    "# vector_size : \n",
    "# workers : 병렬로 처리\n",
    "# sg : CBOW는 0, skip-gram은 0\n",
    "\n",
    "model.wv.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('killed', 0.9599592685699463), ('turks', 0.9569918513298035), ('villages', 0.9568962454795837), ('genocide', 0.9529709815979004), ('armenia', 0.9516713619232178), ('troops', 0.9478585124015808), ('began', 0.9359217286109924), ('land', 0.9322299957275391), ('greece', 0.9317634701728821), ('arms', 0.9315558075904846)]\n",
      "[('jews', 0.9218937158584595), ('war', 0.9064255356788635), ('land', 0.8951766490936279), ('turkey', 0.8849934935569763), ('men', 0.8849554657936096), ('israel', 0.8801628947257996), ('attack', 0.8769087195396423), ('israelis', 0.8650898933410645), ('peace', 0.8613511323928833), ('murder', 0.8612600564956665)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['soldiers']))\n",
    "print(model.wv.most_similar(positive=['man', 'soldiers'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8300685"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=news_texts, window=3, vector_size=100, min_count=5, workers=4, sg=1)\n",
    "# sg=1로만 변경하면 됨\n",
    "\n",
    "model.wv.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('villages', 0.9391025900840759), ('azerbaijanis', 0.9345746636390686), ('wounded', 0.931286096572876), ('azeri', 0.9190162420272827), ('kurds', 0.9106958508491516), ('village', 0.9101150035858154), ('raped', 0.908791184425354), ('murdered', 0.9070382118225098), ('burned', 0.9055033922195435), ('troops', 0.897631049156189)]\n",
      "[('civilians', 0.8298513889312744), ('murders', 0.8295454382896423), ('babies', 0.823814868927002), ('arafat', 0.8204160332679749), ('murder', 0.8184711933135986), ('jew', 0.8148356080055237), ('rape', 0.8130385875701904), ('israelis', 0.8122097253799438), ('wwii', 0.8100835084915161), ('saints', 0.8093860149383545)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['soldiers']))\n",
    "print(model.wv.most_similar(positive=['man', 'soldiers'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 벡터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 09:21:47,502 - word2vec2tensor - INFO - running C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input news_w2v --output naver_w2v\n",
      "2022-02-28 09:21:47,502 - keyedvectors - INFO - loading projection weights from news_w2v\n",
      "2022-02-28 09:21:48,752 - utils - INFO - KeyedVectors lifecycle event {'msg': 'loaded (22220, 100) matrix of type float32 from news_w2v', 'binary': False, 'encoding': 'utf8', 'datetime': '2022-02-28T09:21:48.752945', 'gensim': '4.1.2', 'python': '3.7.11 (default, Jul 27 2021, 09:42:29) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'load_word2vec_format'}\n",
      "2022-02-28 09:21:50,072 - word2vec2tensor - INFO - 2D tensor file saved to naver_w2v_tensor.tsv\n",
      "2022-02-28 09:21:50,072 - word2vec2tensor - INFO - Tensor metadata file saved to naver_w2v_metadata.tsv\n",
      "2022-02-28 09:21:50,074 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model.wv.save_word2vec_format('news_w2v')\n",
    "\n",
    "# 벡터화된 값을 tsv 파일로 저장하기\n",
    "!python -m gensim.scripts.word2vec2tensor --input news_w2v --output naver_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Projector : https://projector.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3d248056fa68818de2d262d28f08c17eec72d9a948bd5f949ae4b63a5944d4e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
