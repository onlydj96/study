{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 분류 등과 같이 별도의 레이블이 없이 텍스트 자체만 있어도 학습이 가능\n",
    "   - Word2Vec의 방식은 주변 관계를 이용하는 것으로 2가지의 방식이 있음\n",
    "   - 1. CBOW(continuous Bag-of-words) : 주변 단어의 임베딩을 더해서 대상단어를 예측\n",
    "   - 2. Skip-Gram : 대상 단어의 임베딩으로 주변단어르 예측\n",
    "      - 일반적으로 CBOW보다 성능이 좋은 편이지만 한번에 여러 단어를 예측해야하기 때문에 비효율적\n",
    "      - 최근에는 negative sampling이라는 방법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-SNE(t-Stochastic Neighbor Embedding)\n",
    "   - T-SNE은 고차원의 벡터들의 구조를 보존하며 저차원으로 사상하는 차원 축소 알고리즘\n",
    "   - 단어 임베딩에서도 생성된 고차원 벡터들을 시각화하기 위해 T-SNE 알고이즘을 많이 이용\n",
    "   - t-sne는 가장 먼저 원 공간의 데이터 유사도와 임베딩 공간의 데이터 유사도를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, \n",
    "                             remove=('headers', 'footers', 'quotes'))   # word2vec하는거라 필요없는 것들을 제거\n",
    "\n",
    "documents = dataset.data\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def clean_text(d):\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', d)\n",
    "    return d\n",
    "\n",
    "# 불용어 제거\n",
    "def clean_stopword(d):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w.lower() for w in d.split() if w not in stop_words and len(w) > 3])\n",
    "\n",
    "def tokenize(d):\n",
    "    return word_tokenize(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11096\n"
     ]
    }
   ],
   "source": [
    "# Pandas Frame으로 변환\n",
    "import pandas as pd\n",
    "news_df = pd.DataFrame({'article':documents})\n",
    "\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.dropna(inplace=True)\n",
    "print(len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "news_df['article'] = news_df['article'].apply(clean_text)\n",
    "\n",
    "# 불용어 제거\n",
    "news_df['article'] = news_df['article'].apply(clean_stopword)\n",
    "\n",
    "# 토크나이저\n",
    "tokenized_news = news_df['article'].apply(tokenize)\n",
    "tokenized_news = tokenized_news.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "drop_news = [index for index, sentence in enumerate(tokenized_news) if len(sentence) <= 1]\n",
    "news_texts = np.delete(tokenized_news, drop_news, axis=0)\n",
    "print(len(news_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim을 이용한 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8143273"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=news_texts, window=3, vector_size=100, min_count=5, workers=4, sg=0)\n",
    "\n",
    "# window : 앞뒤로 몇개의 단어를 보고 유추할 것인가? 보통 3,4\n",
    "# vector_size : \n",
    "# workers : 병렬로 처리\n",
    "# sg : CBOW는 0, skip-gram은 0\n",
    "\n",
    "model.wv.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('genocide', 0.9754312634468079), ('turks', 0.9705201387405396), ('villages', 0.9637882113456726), ('armenia', 0.9602059721946716), ('kurds', 0.9580553770065308), ('turkish', 0.9491828680038452), ('killed', 0.9473462104797363), ('murdered', 0.9434210658073425), ('civilians', 0.9411264657974243), ('armenian', 0.9391229152679443)]\n",
      "[('turkey', 0.8646937012672424), ('jew', 0.8494433760643005), ('sadikov', 0.845048189163208), ('thou', 0.8372932076454163), ('destruction', 0.8336199522018433), ('jews', 0.8334668874740601), ('ismailov', 0.8316466212272644), ('lexicon', 0.8303812742233276), ('armenians', 0.8275583982467651), ('nazi', 0.8263252377510071)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['soldiers']))\n",
    "print(model.wv.most_similar(positive=['man', 'soldiers'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8511009"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=news_texts, window=3, vector_size=100, min_count=5, workers=4, sg=1)\n",
    "# sg=1로만 변경하면 됨\n",
    "\n",
    "model.wv.similarity('man', 'woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('villages', 0.9159388542175293), ('wounded', 0.9120823740959167), ('civilians', 0.9077595472335815), ('massacre', 0.9016125202178955), ('troops', 0.8977922797203064), ('turks', 0.8956711292266846), ('azerbaijanis', 0.8952058553695679), ('raped', 0.8933436870574951), ('kurds', 0.8900163769721985), ('village', 0.889756977558136)]\n",
      "[('fighters', 0.8396631479263306), ('israelis', 0.8285832405090332), ('stalin', 0.8265437483787537), ('grandparents', 0.8197566866874695), ('saints', 0.8162000179290771), ('arafat', 0.8146632313728333), ('wwii', 0.8143231272697449), ('jew', 0.8141732811927795), ('arabs', 0.8132178783416748), ('proofs', 0.8116437792778015)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['soldiers']))\n",
    "print(model.wv.most_similar(positive=['man', 'soldiers'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 벡터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model.wv.save_word2vec_format('news_w2v')\n",
    "\n",
    "# 벡터화된 값을 tsv 파일로 저장하기\n",
    "!python -m gensim.scripts.word2vec2tensor --input news_w2v --output naver_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Projector : https://projector.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3d248056fa68818de2d262d28f08c17eec72d9a948bd5f949ae4b63a5944d4e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
