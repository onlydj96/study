{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규 표현식을 이용한 토큰화\n",
    "   - 한국어도 정규 표현식을 이용해 토큰화 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요', '저는', '자연어', '처리', '를', '배우고', '있습니다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "sentence = \"안녕하세요 ㅋㅋ 저는 자연어 처리(Natural Language Processing)를ㄹ!! 배우고 있습니다.\"\n",
    "\n",
    "tokenizer = RegexpTokenizer('[가-힣]+')\n",
    "\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요 ', ' 저는 자연어 처리(Natural Language Processing)를', '!! 배우고 있습니다.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(\"[ㄱ-ㅎ]+\", gaps=True)\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스를 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence = '성공의 비결은 단 한 가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'\n",
    "\n",
    "text_to_word_sequence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob을 이용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['성공의', '비결은', '단', '한', '가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(sentence)\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of Words(BoW)\n",
    "   - 각 단어들이 몇 개가 포함되어 있는지 ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      - 영어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 2 2 1 1]]\n",
      "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"Think like a man of action and act like man of thought.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())   # 각 단어가 corpus에서의 사용된 횟수(빈도수)\n",
    "print(vector.vocabulary_)   # 딕셔너리 형태로 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 2 1 1]]\n",
      "{'think': 4, 'like': 2, 'man': 3, 'action': 1, 'act': 0, 'thought': 5}\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer(stop_words='english')\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      - 한글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 1 1 1 1]]\n",
      "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(corpus)\n",
    "\n",
    "print(bow.toarray())\n",
    "print(vector.vocabulary_)\n",
    "\n",
    "# 한국어는 스페이스만을 기준으로 구분하기에는 조금 문제가 있다.(형태소 때문에)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from konlpy.tag import Mecab\n",
    "tagger = Mecab('C:/mecab/mecab-ko-dic')\n",
    "\n",
    "corpus = \"평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.\"\n",
    "tokens = tagger.morphs(re.sub(\"(\\.)\", \"\", corpus))\n",
    "\n",
    "vocab = {}\n",
    "bow = []\n",
    "\n",
    "for tok in tokens:\n",
    "    if tok not in vocab.keys():\n",
    "        vocab[tok] = len(vocab)\n",
    "        bow.insert(len(vocab)-1, 1)\n",
    "    else:\n",
    "        index = vocab.get(tok)\n",
    "        bow[index] = bow[index]+1\n",
    "        \n",
    "print(bow)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3d248056fa68818de2d262d28f08c17eec72d9a948bd5f949ae4b63a5944d4e"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
