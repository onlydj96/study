{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화(Tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 특수문자에 대한 처리\n",
    "      - 단어에 일반적으로 사용되는 알파벳, 숫자와는 다르게 특수문자는 별도의 처리가 필요\n",
    "      - 일괄적으로 단어의 특수문자를 제거하는 방법도 있지만 특수문자가 단어에 특별한 의미를 가질 때 이를 학습에 반영시키지 못할 수도 있음\n",
    "      - 특수문자에 대한 일괄적인 제거보다는 데이터의 특성을 파악하고, 처리를 하는 것이 중요\n",
    "    \n",
    "   - 특정 단어에 대한 토큰 분리 방법\n",
    "      - 한 단어지만 토큰으로 분리할 때 판단되는 문자들로 이루어진 'We're, United Kingdom 등의 단어는 어떻게 분리해야 할지 선택이 필요.\n",
    "      - we're은 한 단어이나 분리해도 단어의 의미에 별 영향을 끼치진 않지만 United Kingdom은 두 단어가 모여 특정 의미를 가리켜 분리해선 안됨\n",
    "      - 사용자가 단어의 특성을 고려해 토큰을 분리하는 것이 학습에 유리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 토큰화\n",
    "   - 파이썬 내장 함수인 split을 활용해 단어 토큰화\n",
    "   - 공백을 기준으로 단어를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Time is gold'\n",
    "tokens = [x for x in sentence.split(' ')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 토큰화는 nltk 패키지의 tokenize 모듈을 사용해 손쉽게 구현 가능\n",
    "   - 단어 토큰화는 word_tokenize() 함수를 사용해 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitcamp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time', 'is', 'gold']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 토큰화 \n",
    "   - 문장 토큰화는 줄바꿈 문자('\\n')를 기준으로 문장으로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a beautiful book.\n",
      "But of little use to him who cannot read it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful book.',\n",
       " 'But of little use to him who cannot read it.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = 'The world is a beautiful book.\\nBut of little use to him who cannot read it.'\n",
    "print(sentences)\n",
    "\n",
    "tokens = [x for x in sentences.split('\\n')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 문장 토큰화는 sent_tokenize() 함수를 사용해 구현 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful book.',\n",
       " 'But of little use to him who cannot read it.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokens = sent_tokenize(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 문장 토큰화에서는 온점(.)의 처리를 위해 이진 분류기를 사용할 수도 있음\n",
    "   - 온점은 문장과 문장을 구분해줄 수도, 문장에 포함된 단어를 구성할 수도 있기 때문에 이를 이진 분류기로 분류해 더룩 좋은 토큰화를 구현할 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b2a3e2f2f3becee12495ea02e16c44ac6a87253b3da619be1b2ce1aceff9a27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf270gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
