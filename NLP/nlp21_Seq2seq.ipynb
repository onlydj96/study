{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence-to-Sequence\n",
    "   - Sequence-to-Sequence(Seq2Seq)는 입력된 시퀀스로부터 다른 도메인의 시퀀스를 출력하는 모델이다.\n",
    "   - 예를 들어, 한국어 도메인을 가지는 문장을 입력해 중국어 도메인에 해당하는 문장을 얻을 수 있다.\n",
    "   - Seq2seq는 다른 특별한 기술을 이용하는 것이 아닌 지금까지 배운 RNN 기술들을 조합해 만들며, encoder와 decoder로 구성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계 번역 데이터\n",
    "   - 일반적인 자연어 처리의 경우, 입력 시퀀스와 출력 시퀀스의 길이가 동일함\n",
    "   - Seq2Seq는 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있다고 가정\n",
    "   - Seq2Seq에는 인코더의 입력, 디코더의 입력, 디코더의 출력에 해당하는 데이터가 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 영어와 프랑스어 문장 데이터 : https://www.manythings.org/anki/fra-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import urllib3\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip(압축파일)로 저장되있는 사이트에서 불러와서 저장\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "url = \"https://www.manythings.org/anki/fra-eng.zip\"\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "# 압축풀기\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192341"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas 형식으로 불러오기\n",
    "lines = pd.read_csv(\"./nlp_data/fra.txt\", names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 데이터를 모두 사용할 경우 많은 시간이 소요되기 때문에, 일부 데이터만 사용\n",
    "   - 목표 데이터에는 시작과 끝을 나타내는 토큰이 포함되어야 함\n",
    "   - 여기서는 '\\t'와 '\\n'을 각각 시작과 끝을 나타내는 토큰으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>\\tVa !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>\\tMarche.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>\\tBouge !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>\\tSalut !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>\\tSalut.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\tCours !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\tCourez !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\tPrenez vos jambes à vos cous !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\tFile !\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Run!</td>\n",
       "      <td>\\tFilez !\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    src                                 tar\n",
       "0   Go.                            \\tVa !\\n\n",
       "1   Go.                         \\tMarche.\\n\n",
       "2   Go.                         \\tBouge !\\n\n",
       "3   Hi.                         \\tSalut !\\n\n",
       "4   Hi.                          \\tSalut.\\n\n",
       "5  Run!                         \\tCours !\\n\n",
       "6  Run!                        \\tCourez !\\n\n",
       "7  Run!  \\tPrenez vos jambes à vos cous !\\n\n",
       "8  Run!                          \\tFile !\\n\n",
       "9  Run!                         \\tFilez !\\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000]\n",
    "lines.tar = lines.tar.apply(lambda x: '\\t' + x + '\\n')\n",
    "\n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 해당 예제에서는 글자 단위로 예측, 따라서 글자 집합을 구축해주어야 함\n",
    "   - 구축한 다음, 정렬해 인덱스를 부여해 글자에 해당하는 사전을 만듦\n",
    "   - 사전은 글자를 모델에 투입하도록 변환하거나 예측시 반환되는 인덱스들을 글자로 변환할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in lines.src:\n",
    "    for char in line:\n",
    "        src_vocab.add(char)\n",
    "\n",
    "tar_vocab = set()\n",
    "for line in lines.tar:\n",
    "    for char in line:\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '°': 76, 'é': 77, '’': 78, '€': 79}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, '\\u2009': 100, '\\u200b': 101, '‘': 102, '’': 103, '\\u202f': 104}\n",
      "105\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "\n",
    "src_vocab_size = len(src_vocab) + 1\n",
    "tar_vocab_size = len(tar_vocab) + 1\n",
    "\n",
    "src_to_idx = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_idx = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "\n",
    "print(src_to_idx)\n",
    "print(tar_to_idx)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 인코더에 입력될 입력 데이터를 구성\n",
    "   - 문장의 글자 하나씩을 사전을 이용해 인덱스로 변환해 리스트에 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10], [31, 58, 10]]\n"
     ]
    }
   ],
   "source": [
    "# Encoder input\n",
    "\n",
    "encoder_input = []\n",
    "for line in lines.src:\n",
    "    encoder_input.append([src_to_idx[w] for w in line])\n",
    "    \n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 디코더에 입력될 입력 데이터를 구성\n",
    "   - 문장의 글자 하나씩을 사전을 이용해 인덱스로 변환해 리스트에 넣음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 48, 53, 3, 4, 2], [1, 39, 53, 70, 55, 60, 57, 14, 2], [1, 28, 67, 73, 59, 57, 3, 4, 2], [1, 45, 53, 64, 73, 72, 3, 4, 2], [1, 45, 53, 64, 73, 72, 14, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Decoder input\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    decoder_input.append([tar_to_idx[w] for w in line])\n",
    "    \n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 디코더에 출력과 비교할 목표 데이터를 구성\n",
    "   - 인코더 입력 데이터 처리와 동일하나, 시작 토큰을 제외해주어야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48, 53, 3, 4, 2], [39, 53, 70, 55, 60, 57, 14, 2], [28, 67, 73, 59, 57, 3, 4, 2], [45, 53, 64, 73, 72, 3, 4, 2], [45, 53, 64, 73, 72, 14, 2]]\n"
     ]
    }
   ],
   "source": [
    "# Decoder target\n",
    "\n",
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    decoder_target.append([tar_to_idx[w] for w in line if w != '\\t'])\n",
    "    \n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - 각각의 데이터를 동일한 길이로 맞춰줌\n",
    "  - 길이를 맞춰줄 때는 해당 데이터의 최대 길이로 맞춰줌\n",
    "  - 원 핫 인코딩을 통해 원 핫 벡터로 변환 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 문장 최대길이\n",
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "\n",
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더(Encoder)\n",
    "   - encoder는 입력 문장을 받는 여러개의 RNN cell이 존재\n",
    "   - 입력은 단어 토큰화로 단어 단위로 쪼개지고, 이는 각 시점의 encoder 입력이 된다\n",
    "   - encoder는 모든 단어를 입력받고 마지막 시점의 은닉 상태를 decoder RNN cell의 첫 번째 은닉 상태로 넘겨주며, 이를 컨텍스트 벡터(context vector)이라고 함\n",
    "   - encoder는 입력시퀀스를 컨텍스트 벡터라는 고정 길이 벡터로 압축하고자 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더 모델 구성\n",
    "   - encoder의 구성은 일반 LSTM 모델과 동일\n",
    "   - LSTM 안의 return_state는 은닉 상태를 반환해줘 Seq2seq 모델을 구성할 때 필요함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "\n",
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(256, return_state=True)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더(Decoder)\n",
    "   - decoder는 encoder와 마찬가지로 여러 개의 RNN cell로 이루어져 있음\n",
    "   - decoder의 처음 입력은 시작을 나타내는 토큰을 입력하며, 이 토큰 다음에 등장할 단어를 예측\n",
    "   - 처음 셀에서 단어를 예측한 결과는 다시 다음 시점의 decoder 입력으로 사용\n",
    "   - 위 과정을 반복해, 끝을 나타내는 토큰이 예측될 때까지 반복\n",
    "   - 요약하자면 decoder는 encoder에서 넘겨받은 컨텍스트 벡터를 활용해 시퀀스를 만들어냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교사 강요(Teacher Forcing)\n",
    "  - 앞서 설명한 Seq2seq 모델을 잘 살펴보면 디코더의 입력이 필요하지 않음을 알 수 있음\n",
    "  - 에측이 잘 못 되었을 경우, 잘못된 예측이 다음 시점으로 입력되 연쇄적으로 잘못된 예측을 함\n",
    "  - 이를 해결하기 위해 디코더의 다음 시점의 입력으로 이전 시점의 출력이 아닌, 정답을 주어 이를 방지함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더 모델 구성\n",
    "  - 모델의 구성은 encoder와 거의 유사함\n",
    "  - lstm의 return_sequences는 출력을 시퀀스로 반환할 때 사용\n",
    "  - decoder_lstm을 사용할 때는 initial_state를 인코더의 은닉 상태로 설정\n",
    "  - 마지막으로 Dense layer와 softmax를 통과해 예측 글자에 해당하는 인덱스를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq 모델\n",
    "   - 앞서 구성한 encoder와 decoder를 결합해 seq2seq model을 구성\n",
    "   - 구성한 모델과 준비한 데이터를 사용해 기계 번역 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "375/375 [==============================] - 14s 29ms/step - loss: 0.9064 - val_loss: 0.7903\n",
      "Epoch 2/25\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.5537 - val_loss: 0.6491\n",
      "Epoch 3/25\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.4649 - val_loss: 0.5646\n",
      "Epoch 4/25\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.4133 - val_loss: 0.5180\n",
      "Epoch 5/25\n",
      "375/375 [==============================] - 10s 28ms/step - loss: 0.3775 - val_loss: 0.4784\n",
      "Epoch 6/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.3514 - val_loss: 0.4551\n",
      "Epoch 7/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.3294 - val_loss: 0.4332\n",
      "Epoch 8/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.3118 - val_loss: 0.4201\n",
      "Epoch 9/25\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.2974 - val_loss: 0.4074\n",
      "Epoch 10/25\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.2853 - val_loss: 0.3980\n",
      "Epoch 11/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2751 - val_loss: 0.3896\n",
      "Epoch 12/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2660 - val_loss: 0.3888\n",
      "Epoch 13/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2581 - val_loss: 0.3814\n",
      "Epoch 14/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2510 - val_loss: 0.3771\n",
      "Epoch 15/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2444 - val_loss: 0.3735\n",
      "Epoch 16/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2385 - val_loss: 0.3714\n",
      "Epoch 17/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2330 - val_loss: 0.3713\n",
      "Epoch 18/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2279 - val_loss: 0.3705\n",
      "Epoch 19/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2231 - val_loss: 0.3693\n",
      "Epoch 20/25\n",
      "375/375 [==============================] - 10s 27ms/step - loss: 0.2186 - val_loss: 0.3682\n",
      "Epoch 21/25\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.2143 - val_loss: 0.3660\n",
      "Epoch 22/25\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.2103 - val_loss: 0.3656\n",
      "Epoch 23/25\n",
      "375/375 [==============================] - 8s 22ms/step - loss: 0.2064 - val_loss: 0.3667\n",
      "Epoch 24/25\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.2028 - val_loss: 0.3667\n",
      "Epoch 25/25\n",
      "375/375 [==============================] - 10s 26ms/step - loss: 0.1995 - val_loss: 0.3692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db321a87c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input], y=decoder_target, \n",
    "          batch_size=128, \n",
    "          epochs=25,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 \n",
    "   - 일반 모델과는 다르게, seq2seq 모델은 모델 예측 프로세스가 다름\n",
    "   - 예측할 때는 인덱스 하나씩을 예측하게 되며, 예측한 인덱스를 저장하고 이를 다시 입력으로 사용해 사용해 종료 토큰이 나올 때까지 반복\n",
    "   - 마지막으로 예측한 인덱스들을 사전을 통해 글자들로 변환해 최종 예측을 얻음?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(256))\n",
    "decoder_state_input_c = Input(shape=(256))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                      outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_src = dict((i, char) for char, i in src_to_idx.items())\n",
    "idx_to_tar = dict((i, char) for char, i in tar_to_idx.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_decode(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_idx['\\t']] = 1\n",
    "    \n",
    "    stop = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    while not stop:\n",
    "         output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "         \n",
    "         sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "         sampled_char = idx_to_tar[sampled_token_index]\n",
    "         decoded_sentence += sampled_char\n",
    "         \n",
    "         if sampled_char == '\\n' or len(decoded_sentence) > max_tar_len:\n",
    "             stop = True\n",
    "             \n",
    "         target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "         target_seq[0, 0, sampled_token_index] = 1\n",
    "         \n",
    "         states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 :  Go.\n",
      "정답 :  Va !\n",
      "번역 :  Décampe ! \n",
      "\n",
      "입력 :  Go.\n",
      "정답 :  Marche.\n",
      "번역 :  Décampe ! \n",
      "\n",
      "입력 :  Go.\n",
      "정답 :  Bouge !\n",
      "번역 :  Décampe ! \n",
      "\n",
      "입력 :  Hi.\n",
      "정답 :  Salut !\n",
      "번역 :  Sais ! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in [0, 1, 2, 3]:\n",
    "    input_seq = encoder_input[seq_index : seq_index+1]\n",
    "    decoded_sentence = predict_decode(input_seq)\n",
    "    \n",
    "    print(\"입력 : \", lines.src[seq_index])\n",
    "    print(\"정답 : \", lines.tar[seq_index][1:len(lines.tar[seq_index])-1])\n",
    "    print(\"번역 : \", decoded_sentence[:len(decoded_sentence)-1], '\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3caf13703c5b1c02abff9fa597e671e1239d1d668b6a345ae62ddadff9d8fc63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
