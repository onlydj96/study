{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개체명 인식 훈련(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비\n",
    "   - https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14041\n",
      "[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = []\n",
    "sentence = []\n",
    "\n",
    "with urllib.request.urlopen('https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.decode('utf-8')\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] ==\"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                tagged_sentences.append(sentence)\n",
    "                sentence=[]\n",
    "            continue\n",
    "        splits = line.strip().split(' ')\n",
    "        word = splits[0].lower()\n",
    "        sentence.append([word, splits[-1]])\n",
    "print(len(tagged_sentences))\n",
    "print(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 개체명 태그를 분리해서 데이터를 구성\n",
    "\n",
    "sentences, ner_tags = [], []\n",
    "\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tag_info = zip(*tagged_sentence)\n",
    "    sentences.append(list(sentence))\n",
    "    ner_tags.append(list(tag_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정제 및 빈도 수가 높은 상위 단어들만 추출하기 위해 토큰화 작업\n",
    "\n",
    "max_words = 4000\n",
    "src_tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')\n",
    "src_tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 10\n"
     ]
    }
   ],
   "source": [
    "vocab_size = max_words\n",
    "tag_size = len(tar_tokenizer.word_index) + 1\n",
    "\n",
    "print(vocab_size, tag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 학습하기 위해서 데이터를 배열로 변환\n",
    "\n",
    "x_train = src_tokenizer.texts_to_sequences(sentences)\n",
    "y_train = tar_tokenizer.texts_to_sequences(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding (문장에 길이를 맞춰줌)\n",
    "\n",
    "max_len = 70\n",
    "x_train = pad_sequences(x_train, padding='post', maxlen=max_len)\n",
    "y_train = pad_sequences(y_train, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11232, 70) (11232, 70, 10) (2809, 70) (2809, 70, 10)\n"
     ]
    }
   ],
   "source": [
    "# train과 test 분리, 원핫인코딩\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, y_train,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=66)\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = to_categorical(y_test, num_classes=tag_size)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "88/88 [==============================] - 10s 48ms/step - loss: 0.1851 - accuracy: 0.8238 - val_loss: 0.1258 - val_accuracy: 0.8276\n",
      "Epoch 2/5\n",
      "88/88 [==============================] - 2s 23ms/step - loss: 0.0976 - accuracy: 0.8567 - val_loss: 0.0783 - val_accuracy: 0.8797\n",
      "Epoch 3/5\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 0.0644 - accuracy: 0.9067 - val_loss: 0.0544 - val_accuracy: 0.9200\n",
      "Epoch 4/5\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 0.0454 - accuracy: 0.9370 - val_loss: 0.0424 - val_accuracy: 0.9396\n",
      "Epoch 5/5\n",
      "88/88 [==============================] - 2s 21ms/step - loss: 0.0347 - accuracy: 0.9511 - val_loss: 0.0383 - val_accuracy: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2912e799948>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88/88 [==============================] - 1s 9ms/step - loss: 0.0384 - accuracy: 0.9474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03842393308877945, 0.9473790526390076]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = src_tokenizer.index_word\n",
    "idx2ner = tar_tokenizer.index_word\n",
    "idx2ner[0] = 'PAD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어             |실제값  \n",
      "----------------------------------\n",
      "no               : O       O\n",
      "one              : O       O\n",
      "OOV              : O       O\n",
      "from             : O       O\n",
      "OOV              : O       O\n",
      ".                : O       O\n",
      "\"                : O       O\n"
     ]
    }
   ],
   "source": [
    "# 원하는 인덱스에서 예측값과 실제값 비교\n",
    "\n",
    "i = 70\n",
    "y_predict = model.predict(np.array([x_test[i]]))\n",
    "y_predict = np.argmax(y_predict, axis=-1)\n",
    "true = np.argmax(y_test[i], -1)\n",
    "\n",
    "print(\"{:15}|{:5}\".format(\"단어\", \"실제값\", \"예측값\"))\n",
    "print(\"-\" * 34)\n",
    "\n",
    "for w, t, pred in zip(x_test[i], true, y_predict[0]):\n",
    "    if w != 0:\n",
    "        print(\"{:17}: {:7} {}\".format(idx2word[w], idx2ner[t].upper(), idx2ner[pred].upper()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3caf13703c5b1c02abff9fa597e671e1239d1d668b6a345ae62ddadff9d8fc63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
