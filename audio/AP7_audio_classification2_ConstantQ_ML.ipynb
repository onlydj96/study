{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant-Q를 이용한 머신러닝 오디오 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비\n",
    "   - wav는 매 순간의 음압을 측정하여 그 수치를 저장한 형태이기 때문에 그 자체로 음악을 분석하기에 적합하지 않음(음의 높이와 세기를 듣는것이지 순간의 음압을 듣는게 아니기 때문)\n",
    "   - 푸리에 변환과 같은 변환 기법을 이용하여 시간 축의 데이터를 주파수 축의 데이터로 바꿔줘야할 필요가 있음\n",
    "   - 푸리에 변환 대신 푸리에 변환과 유사한 Constant-Q 변환을 사용\n",
    "   - Constant-Q 변환은 주파수 축이 로그 단위로 변환되고, 각 주파수에 따라 해상도가 다양하게 처리되기 때문에(저주파는 저해상도, 고주파는 고해상도) 음악을 처리하는 데에 푸리에 변환보다 유리\n",
    "   - 주파수 대역을 저장할 리스트 audio_cqt 선언\n",
    "   - constant-Q 변환할 때는 변환할 오디오 데이터와 sampling rate가 필요\n",
    "   - 해당 데이터에서는 sampling rate가 모두 동일하므로 따로 처리가 필요하지 않음\n",
    "   - 여기서는 Constant-Q 변환을 사용해 오디오 데이터를 주파수 대역으로 변환\n",
    "\n",
    "   - 변환에는 앞서 준비한 데이터를 가져와 사용하며, Constant-Q 변환에는 librosa.cqt 함수를 사용\n",
    "   - 여기서 n_bins는 옥타브 단계 및 개수를, bins_per_octave는 한 옥타브가 가지는 단계를 의미\n",
    "   - 라벨에 대해선 원 핫 인코딩을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import librosa\n",
    "import librosa.display \n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file = './GeneralMidi.wav'\n",
    "instruments = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "num_notes = 50\n",
    "sec = 2\n",
    "\n",
    "audio = []\n",
    "inst = []\n",
    "for inst_idx, note in itertools.product(range(len(instruments)), range(num_notes)):\n",
    "    instrument = instruments[inst_idx]\n",
    "    offset = (instrument*num_notes*sec + (note*sec))\n",
    "    print('instrument : {}, note: {}, offset : {}'.format(instrument, note, offset))\n",
    "    y, sr = librosa.load(midi_file, sr=None, offset=offset, duration=2.0)\n",
    "    audio.append(y)\n",
    "    inst.append(inst_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_cqt = []\n",
    "for y in audio:\n",
    "    ret = librosa.cqt(y, sr, hop_length=1024, n_bins=24*7, bins_per_octave=24)\n",
    "    ret = np.abs(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 앞서 생성한 주파수 대역을 spectrogram을 시각화\n",
    "   - 악기 간 spectrogram을 비교해보면 차이가 존재함을 알 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(instruments)*num_notes, num_notes):\n",
    "    amp_db = librosa.amplitude_to_db(np.abs(audio_cqt[i]), ref=np.max)\n",
    "    librosa.display.specshow(amp_db, sr=sr, x_axis='time', y_axis='cqt_note')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Instrument : {}'.format(inst[i]))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 훈련 데이터와 실험 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cqt_np = np.array(audio_cqt, np.float32)\n",
    "inst_np = np.array(inst, np.int16)\n",
    "\n",
    "print(cqt_np.shape, inst_np.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 분류기에서 사용하기 위해 3차원 벡터를 2차원 벡털 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cqt_np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3832/3367321419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcqt_np\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcqt_np\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m168\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m87\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cqt_np' is not defined"
     ]
    }
   ],
   "source": [
    "cqt_np = cqt_np.reshape((500, 168 * 87))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 읽어온 데이터는 음량이나 범위가 다를 수 있음\n",
    "   - min-max scaling을 통해 데이터의 범위를 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(cqt_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - 학습 데이터와 실험 데이터를 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(cqt_np, inst_np, test_size=0.2)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(train_x, train_y)\n",
    "pred = LR.predict(test_x)\n",
    "acc = accuracy_score(pred, test_y)\n",
    "print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(kernel='linear')\n",
    "SVM.fit(train_x, train_y)\n",
    "pred = SVM.predict(test_x)\n",
    "acc = accuracy_score(pred, test_y)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(train_x, train_y)\n",
    "pred = DT.predict(test_x)\n",
    "acc = accuracy_score(pred, test_y)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a911b5aa042724ead966489e6f5164935952a049f40d7a798c17a648930e160"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
