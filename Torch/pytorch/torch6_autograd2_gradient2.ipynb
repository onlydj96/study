{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동 미분 흐름 다시보기(1)\n",
    "   - 계산 흐름\n",
    "   a -> b -> c -> out\n",
    "\n",
    "   - backward()를 통해\n",
    "   a <- b <- c<- out을 계산하면 미분값이 a.grad에 채워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2, requires_grad=True)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.data:  tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "a.grad :  tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "a.grad_fn :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"a.data: \", a.data)\n",
    "print(\"a.grad : \", a.grad)         # 계산이 이뤄져야 결과치가 반환됨\n",
    "print(\"a.grad_fn : \", a.grad_fn)   # 계산이 이뤄져야 결과치가 반환됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - b = a + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "b = a + 2\n",
    "print(b)        # add로 연산 grad_fn=<AddBackward0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - c = b ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = b ** 2\n",
    "print(c)   # 제곱 연산 grad_fn=<PowBackward0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = c.sum()\n",
    "print(out)   # grad_fn=<SumBackward0>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   a, b, c의 data, grad, grad_fn, out 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.data:  tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "a.grad :  tensor([[6., 6.],\n",
      "        [6., 6.]])\n",
      "a.grad_fn :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"a.data: \", a.data)\n",
    "print(\"a.grad : \", a.grad)\n",
    "print(\"a.grad_fn : \", a.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.data:  tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "b.grad :  None\n",
      "b.grad_fn :  <AddBackward0 object at 0x00000153D47202B0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(\"b.data: \", b.data)\n",
    "print(\"b.grad : \", b.grad)\n",
    "print(\"b.grad_fn : \", b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c.data:  tensor([[9., 9.],\n",
      "        [9., 9.]])\n",
      "c.grad :  None\n",
      "c.grad_fn :  <PowBackward0 object at 0x00000153D47206A0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(\"c.data: \", c.data)\n",
    "print(\"c.grad : \", c.grad)\n",
    "print(\"c.grad_fn : \", c.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동 미분 흐름 다시보기(2)\n",
    "   - grad값을 넣어서 backward\n",
    "   - 아래의 코드에서 .grad값이 None은 gradient값이 필요하지 않기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = (x ** 2)\n",
    "z = y ** 2 + x\n",
    "out = z.sum()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = torch.Tensor([0.1, 1, 100])\n",
    "z.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.data:  tensor([1., 1., 1.])\n",
      "x.grad :  tensor([  0.5000,   5.0000, 500.0000])\n",
      "x.grad_fn :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"x.data: \", x.data)\n",
    "print(\"x.grad : \", x.grad)\n",
    "print(\"x.grad_fn : \", x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.data:  tensor([1., 1., 1.])\n",
      "y.grad :  None\n",
      "y.grad_fn :  <PowBackward0 object at 0x00000153E985FF40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "print(\"y.data: \", y.data)\n",
    "print(\"y.grad : \", y.grad)\n",
    "print(\"y.grad_fn : \", y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.data:  tensor([1., 1., 1.])\n",
      "z.grad :  tensor([  0.5000,   5.0000, 500.0000])\n",
      "z.grad_fn :  None\n"
     ]
    }
   ],
   "source": [
    "print(\"z.data: \", x.data)\n",
    "print(\"z.grad : \", x.grad)\n",
    "print(\"z.grad_fn : \", x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b69a98d3df882577ba469635c4ab08c5ae67eaedfd3a57f311f98966a6edb2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
