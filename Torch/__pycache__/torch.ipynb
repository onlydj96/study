{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    1. img_size : int \n",
    "       Size of the image (it is a square).\n",
    "    \n",
    "    2. patch_size : int\n",
    "       Size of tje patch (it is a square).\n",
    "      \n",
    "    3. in_chans : int\n",
    "       Number of input channels.\n",
    "    \n",
    "    4. embed_dim : int\n",
    "       the embedding dimension.\n",
    "       \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    1. n_patched : int\n",
    "       Number of patched inside of our image.\n",
    "    \n",
    "    2. proj : nn.Conv2d\n",
    "       Convolutoinal layer that does both the splitting into patches and their embedding.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=786):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(\n",
    "                in_chans,\n",
    "                embed_dim,\n",
    "                kernel_size=patch_size,\n",
    "                stride=patch_size,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            shape '(n_samples, in_chans, img_size, img_size)'.\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "              shape '(n_samples, n_patches, embed_dim)'.\n",
    "\n",
    "        '''\n",
    "        \n",
    "        x = self.proj(x)   # (n_samples, embed_dim, n_patches ** 0.5, n_patched ** 0.5) \n",
    "        \n",
    "        x = x.flatten(2)   # (n_samples, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)   # (n_samples, n_patches, embed_dim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention mechanism.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    1. dim : int\n",
    "       the input and out dimension of per token features.\n",
    "    \n",
    "    2. n_heads : int\n",
    "       Number of attention heads \n",
    "       \n",
    "    3. qkv_bias : bool\n",
    "       If True then we include bias to the query, key and value projections\n",
    "       \n",
    "    4. attn_p : float\n",
    "       Dropout probility applied to the query, key and value tensors.\n",
    "       \n",
    "    5. proj_p : float\n",
    "       Dropout probability applied to the output tensor.\n",
    "       \n",
    "    Attributes\n",
    "    -----------\n",
    "    \n",
    "    scale : float\n",
    "       Normalizing consant for the dot product.\n",
    "       \n",
    "    qkv : nn.linear\n",
    "       Linear projection for the query, key and value\n",
    "       \n",
    "    proj : nn.Linear\n",
    "       Linear mappin that takes in the concatenated output of all attention heads and maps it into a new space.\n",
    "       \n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "       Dropout layers.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            shape '(n_samples, n_patches + 1, dim)'.\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "              shape '(n_samples, n_patches + 1, dim)'.\n",
    "\n",
    "        '''\n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        x = self.proj(x)   # (n_samples, embed_dim, n_patches ** 0.5, n_patched ** 0.5) \n",
    "        \n",
    "        if dim != self.dim:\n",
    "            raise ValueError\n",
    "        \n",
    "        qkv = self.qkv(x) # (n_samples. n_patches +1, 3 * dim)\n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim) # (n_samples, n_patches +1, 3, n_heads, head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  # (3, n_samples, n_heads, n_patches +1, head_dim)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k_t = k.transpose(-2, -1)   # (n_samples, n_heads, head_dim, n_patches+1)\n",
    "        dp = (q @ k_t) * self.scale  # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
    "        attn = dp.softmax(dim=-1)  # (n_samples, n_heads, n_patches+1, n_patches+1)\n",
    "        \n",
    "        weighted_avg = attn @ v  # (n_samples, n_heads, n_patches+1, head_dim)\n",
    "        weighted_avg = weighted_avg.transpose(1, 2)   # (n_samples, n_patches+1, n_heads, head_dim)\n",
    "        weighted_avg = weighted_avg.flatten(2)  # (n_samples, n_patches+1, dim)\n",
    "        \n",
    "        x = self.proj(weighted_avg) # (n_samples, n_patches+1, dim)\n",
    "        x = self.proj_drop(x)  # (n_samples, n_patches+1, dim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer perceptron\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    1. in_features : int\n",
    "       Number of input features.\n",
    "       \n",
    "    2. hidden_features : int\n",
    "       Number of nodes in the hidden layer\n",
    "       \n",
    "    3. out_features : int\n",
    "       Number of output features.\n",
    "    \n",
    "    4. p : float\n",
    "       Dropout probability.\n",
    "       \n",
    "    Attributes\n",
    "    ---------------\n",
    "    1. fc : nn.Linear\n",
    "       the first linear layer.\n",
    "       \n",
    "    2. act : nn.GELU\n",
    "       GELU activation fuction.\n",
    "       \n",
    "    3. fc2 : nn.linear\n",
    "       the second linear layer.\n",
    "    \n",
    "    4. drop : nn.Dropout\n",
    "       Dropout layer.      \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, n_patches +1, in_features)'\n",
    "        \n",
    "        Returns\n",
    "        ---------\n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches+1, out_features)'\n",
    "        '''\n",
    "        x = self.fc1(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.drop(x)  # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, n_patches+1, hidden_features)\n",
    "        x - self.drop(x)  # (n_samples, n_patches+1, hidden_features)\n",
    "        \n",
    "        return x\n",
    "class Block(nn.Module):\n",
    "    '''\n",
    "    Transformer block\n",
    "    \n",
    "    Parameters\n",
    "    -------------\n",
    "    1. dim : int\n",
    "       Embedding dimension\n",
    "    \n",
    "    2. n_heads : int\n",
    "       Number of attention heads.\n",
    "       \n",
    "    3. mlp_ratio : float\n",
    "       Determines the hidden dimension size of the 'MLP' module with respect to 'dim'.\n",
    "       \n",
    "    4. qkv_bias : bool\n",
    "       If True then we include bias to the query, key and value projections.\n",
    "       \n",
    "    5. p, attn_p : float \n",
    "       Dropout probability\n",
    "       \n",
    "    Attributes\n",
    "    1. norm1, norm2 : LayerNorm\n",
    "       Layer normalization\n",
    "       \n",
    "    2. attn : Attention\n",
    "       Attention module.\n",
    "       \n",
    "    3. mlp : MLP\n",
    "       MLP module\n",
    "    '''\n",
    "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.attn = Attention(dim, \n",
    "                              n_heads=n_heads,\n",
    "                              qkv_bias=qkv_bias, \n",
    "                              attn_p=attn_p, \n",
    "                              proj_p=p)\n",
    "        \n",
    "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features=dim, \n",
    "                       hidden_features=hidden_features,\n",
    "                       out_features=dim,)\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run forward pass.\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        x : torch.Tensor\n",
    "           Shape '(n_samples, n_patches+1, dim)'.\n",
    "           \n",
    "        Returns\n",
    "        --------\n",
    "        torch.Tensor\n",
    "           Shape '(n_samples, n_patches+1, dim)'.\n",
    "        '''\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x \n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    '''\n",
    "    Simplified implementation of the Vision transformer.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    1. img_size : int\n",
    "       both height and the width of the image (it is a sqaure)\n",
    "    \n",
    "    2. patch_size : int \n",
    "       Both height and the width of the patch (it is a square).\n",
    "       \n",
    "    3. in_chans : int\n",
    "       Number of input channels\n",
    "       \n",
    "    4. n_classes : int\n",
    "       Number of classes.\n",
    "       \n",
    "    5. embed_dim : int\n",
    "       Dimensionality of the token/patch embeddings.\n",
    "       \n",
    "    6. depth : int \n",
    "       Number of blocks\n",
    "       \n",
    "    7. n_heads : int\n",
    "       Number of attention heads.\n",
    "       \n",
    "    8. mlp_ratio : float\n",
    "       Determines the hidden dimension of the 'MLP' module.\n",
    "       \n",
    "    9. qkv_bias : bool\n",
    "       If True then we include bias to the query, key and value projections.\n",
    "       \n",
    "    10. p, attn_p : float\n",
    "       Dropout probability \n",
    "       \n",
    "    Attributes\n",
    "    -----------\n",
    "    patch_embed : PatchEmbed \n",
    "       Instance of 'PatchEmbed' layer.\n",
    "       \n",
    "    cls_token : nn.Parameter\n",
    "       Learnable parameter that will represent the first token in the sequence.\n",
    "       \n",
    "    pos_emb : nn.Parameter\n",
    "       Positional embedding of the cls token + all the patches. \n",
    "       It has '(n_patches + 1) * embed_dim' elements.\n",
    "    \n",
    "    pos_drop : nn.Dropout\n",
    "       Dropout layer.\n",
    "       \n",
    "    blocks : nn.ModuleList\n",
    "       List of 'Block' modules.\n",
    "       \n",
    "    norm : nn.LayerNorm\n",
    "       Layer normalization.      \n",
    "    '''\n",
    "    def __init__(self, img_size=384, \n",
    "                 patch_size=16,\n",
    "                 in_chans=3,\n",
    "                 n_classes=1000,\n",
    "                 embed_dim=768,\n",
    "                 depth=12,\n",
    "                 n_heads=12,\n",
    "                 mlp_ratio=4,\n",
    "                 qkv_bias=True,\n",
    "                 p=0,\n",
    "                 attn_p=0.,):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size,\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=in_chans,\n",
    "                                      embed_dim=embed_dim,)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.pod_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList([Block(dim=embed_dim,\n",
    "                                           n_heads=n_heads,\n",
    "                                           mlp_ratio=mlp_ratio,\n",
    "                                           qkv_bias=qkv_bias,\n",
    "                                           p=p,\n",
    "                                           attn_p=attn_p,)\n",
    "                                     for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.head = nn.Linear*(embed_dim, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run the forward pass\n",
    "        \n",
    "        Parameters \n",
    "        -------------\n",
    "        x : torch.Tensor\n",
    "           Shape '(n_samples, in_chans, img_size, img_size)'.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        logits : torch.Tensor\n",
    "           Logits over all the classes - '(n_samples, n_classes)'.\n",
    "        '''\n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1)  # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = x + self.pos_embed  # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        cls_token_final = x[:,0]  # just the CLS token\n",
    "        x = self.head(cls_token_final)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: Could not find module 'C:\\ProgramData\\Anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8768/3333258613.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcustom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVsionTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'custom'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b69a98d3df882577ba469635c4ab08c5ae67eaedfd3a57f311f98966a6edb2d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
